# 0. Contents Table
* [1. Server Development](#1-Server-Development)
  + [1.1 System Architecture](#11-System-Architecture)
  + [1.2 Auth](#12-Auth)
  + [1.3 Lobby](#13-Lobby)
  + [1.4 Game](#14-Game)
* [2. Infrastructure](#2-Infrastructure)
  + [2.1 Statistics System](#21-Statistics-System)
  + [2.2 Log System](#22-Log-System)
  + [2.3 Monitoring System](#23-Monitoring-System)
* [3. DevOps](#3-DevOps)
  + [3.1 CI](#31-CI)
  + [3.2 CD](#32-CD)
  + [3.3 GStar](#33-GStar)

# 1. Server Development
## 1.1 Server Architecture
* 경력 기술서 내용의 이해를 돕기 위한 간단한 서버 구성도

![server2](https://github.com/user-attachments/assets/9d7fd7c4-c20b-4a51-9c4c-1d51fc2b3576)

## 1.2 Auth
* (인증 서버는 초기 설계부터 구현 및 운영까지 작성자가 모두 작업함)
### 작업 내용
* 사용자가 외부에서 사용하는 플랫폼 계정을 게임 내 계정(account id)과 바인딩하여 외부 플랫폼 계정으로 게임을 플레이할 수 있게 지원
  + 연동한 외부 플랫폼은 사내 계정 관리 시스템(nano)과 스팀이 있음
* 인증 서버로부터 사용자가 인증된 클라이언트에게 인증 서버 외 다른 서버나 외부 시스템에서 쉽게 인증(권한 부여)할 수 있도록 내부 토큰(LLL Token) 시스템 지원
  + 클라이언트는 지급 받은 내부 토큰을 다른 서버(로비)에게 제출하여 간단한 인증 수행
* 내부(자체) 계정 시스템 지원
* 처음에 Golang + Redis로 만들었으나 (auth1) 자체 서버 프레임워크(C/C++) 기반으로 다시 만듦 (auth2)
### Trouble shooting
<details>
<summary>Trouble shooting 사례 보기</summary>

> #### 개발 환경에서의 멀티 클라이언트 및 멀티 로그인 지원을 위한 내부 계정 시스템 도입
> * 클라이언트/서버 개발자들이 하나의 로컬 PC에서 여러 개의 클라이언트를 실행하여 테스트 진행하는 경우가 잦았음
> * 사내 계정(nano)은 1인 1개씩 부여되므로 1개 nano 계정으로 밖에 로그인을 할 수 밖에 없는 상황
> * 하지만 게임 규칙 상 1개 클라이언트는 1개 계정으로만 플레이가 가능하고 서버 기술적으로도 여러개 클라이언트가 1개의 계정을 동시에 사용하는 경우는 지원되지 않았음
> * 따라서 1인당 다수의 계정이 필요한 상황임이 인지되었고 인증 서버를 개발하여 오픈하기 전에 내부 계정 시스템을 도입
> * 인증 db에 nano 계정과 게임 계정(account id)을 바인딩하는 테이블 외에 자체 계정(id/pw)을 저장하는 테이블을 추가
> * (내부)계정 관리 툴(웹 서비스)에 사용자가 직접 nano 계정으로 접근하여 내부 계정을 생성하고 지급 받음
>   + 계정 관리 툴 + 계정 관리 서비스는 별도 담당자가 작업 및 관리
>   + 계정 관리 서비스에서 인증 db에 접근하여 id/pw를 추가하거나 삭제 등의 동작 지원
> * 인증 서버는 OAuth 토큰 기반 외부 IdP(nano 및 스팀) 인증 방법 외에 내부 계정 인증 서비스를 제공
> #### 내부 계정 토큰화 지원
> * 내부 계정은 id/pw 형식이며 클라이언트에서 직접 입력하여 로그인할 수도 있지만 별도 파일에 저장하여 자동 로그인 시에도 사용 가능
>   + 자동 로그인은 사용자가 직접 id/pw를 입력할 필요 없이 클라이언트가 계정이 저장된 파일로부터 계정을 읽어들여 로그인하는 기능
> * 하지만 이로 인해 계정이 평문 그대로 파일에 노출되는 것과 id/pw를 한번 발급하면 관리(특히 회수/무효화)가 어렵다는 문제가 생김
> * 따라서 평문 노출을 없애고 발급한 계정의 만료 기간을 설정하기 위해 내부 토큰화 기능을 지원
>   + 사용자는 계정 관리 툴에서 계정을 생성한 후 토큰화를 추가로 수행하여 토큰이 저장된 파일을 로컬 PC에 다운로드
>     + id/pw를 토큰 생성 시 토큰 필드에 추가하여 encryption
>   + 클라이언트가 자동 로그인 시 다운로드한 파일로부터 토큰을 읽어들여 로그인 시도
>   + 계정 관리 서버(python flask)에서 토큰화는 인증 서버가 사용하는 토큰 라이브러리(dll)를 로드하여 토큰을 생성하고 발급
>   + 토큰은 만료시점 필드가 내부에 저장되어 있으며 인증 서버가 토큰 해석 시 토큰이 만료되었으면 로그인을 거절
> #### 내부 토큰 암호화 알고리즘의 AES 256 CBC -> AES 256 ECB 전환
> * OAuth 토큰 해석법으로, 토큰 발급자에게 토큰을 전달하여 해당 토큰의 정보를 질의하거나 토큰을 직접 해석하는 방법이 있음
> * 인증 서버는 기본적으로 토큰을 직접 해석하는 방법을 지원
> * 따라서 토큰을 생성하고 해석하는 별도 라이브러리(dll, token)를 토큰 관련 작업이 필요한 서버들이 로드하여 토큰을 생성하고 암호화하거나 복호화 수행
> * 토큰은 JWT 포맷을 따르며 암호화 알고리즘으로 AES 256 ECB를 사용
>   + cbc(cipher block chaining)를 사용하지 않고 ecb(electronic code book)를 사용한 이유는 cbc 특성상 이전에 암호화된 블럭을 (다시 XOR하여) IV로 사용하기 때문에 토큰간 의존성(순서)이 생김
>   + 따라서 스트림 데이터에 쓰이는 더욱 강력한 암호화 알고리즘인 cbc를 사용하는 대신 조금 덜 보안적이지만 의존성을 없애기 위해 ecb 사용
>   + 토큰 질의(introspection) 서비스는 외부 시스템에서 사용할 수 있게 제공할 예정이었으나 이 부분 작업 진행은 우선순위 이슈로 홀드된 상태
> * JWT 토큰 암호화는 HS256 사용
> * 토큰 암호화 키와 HMAC 키는 별개이며 모두 소스코드에 박혀져 있음
>   + 소스코드 접근은 일부 인원에게만 허용됨
> * 토큰 암호화/복호화를 위해 openssl3 라이브러리 사용
> #### 토큰 vs 세션
> * LLL 시스템은 토큰과 세션 모두를 채용
>   + 보통 토큰과 세션은 대조되는 개념으로 취급됨
>   + 사용자 인증 관련한 부분은 토큰을 사용하고 내부 시스템에서 좀 더 복잡한 유저의 상태 관리는 세션을 사용

</details>

## 1.3 Lobby
* (서버들의 출시를 위한 scale out이 가능한 구조로의 개선이 지상 과제가 되면서 작성자가 아래 정리한 작업 내용을 작업함)
### 작업 내용
#### Login
* 로비 서버가 자체 세션 서버(C/C++)와 연동하여 유저 세션을 관리하던 부분을 Redis++을 사용하여 Redis와 연동하도록 수정
  + Redis++을 wrapping한 별도 라이브러리(dll, memory database component)를 구현 후 로비 서버에서 로드하여 사용
* 위 작업으로 인한 유저 로그인 처리 로직 대부분을 수정
#### Nickname & Character
* 인게임에서 사용할 닉네임과 캐릭터 관련 준비 작업(In-game Preparation)하는 부분이 원래 로비 서버가 별도 서버들과 연동하여 수행했던 부분들을 모두 라이브러리화하여 연동하도록 수정
  + 로비에서 닉네임 생성/수정 또는 캐릭터 생성/삭제 등의 작업이 가능함
  + 계정 서버와 캐릭터 서버가 별도로 존재했었음 (당시 서버 구조가 MSA를 추구)
  + 별도 서버들을 모두 라이브러리(dll)로 변환하여 로비 서버에서 로드하고 사용하도록 수정
#### Inter-server Communication 
* 서버들 사이의 연결 복잡도를 줄이고 통신 방법을 표준화하기 위하여 메시지 큐(Kafka) 도입
  + 근실시간 동기화 수준 정도면 충족되는 서버들 사이의 연결에 한함
  + memory database component와 마찬가지로 librdkafka 라이브러리를 wrapping하여 별도 라이브러리(dll, message queue client component)를 구현 후 로비 서버에서 로드하여 사용
### Trouble shooting
<details>
<summary>Trouble shooting 사례 보기</summary>
  
> #### Component dll 로드 시 ABI 문제
> * 앞서 설명한 memory database component, message queue client component 등 오픈소스 라이브러리를 한번 wrapping하여 다시 dll로 뽑아냄
>   + 오픈소스의 필요한 기능만 제한적으로 사용하도록, 일관적인 코드 사용 등을 위해 wrapping함
> * 서버 빌드 시 컴파일러 버전과 옵션 등의 세팅을 모든 개발자 PC에서 동일하게 사용하도록 빌드 환경 및 시스템이 구축되어 있어서 C++ 요소들을 그대로 dll export하여도 문제가 되지 않았음
>   + 위에서 설명한 서버 구성도의 모든 서버들은 C/C++로 작성된 공통의 서버 프레임워크(dll)를 사용함
> * 하지만 vcpkg로 다운받은 redis++ 라이브러리나 librdkafka 라이브러리를 wrapping한 component 라이브러리를 서버에서 로드하여 사용할 때 ABI 문제가 발생
>   + component 라이브러리의 소스코드가 vcpkg 라이브러리 함수 호출 후 반환값(문자열 등)에 접근하면 크래시 발생
>   + 오픈소스라서 직접 소스코드를 다운받고 빌드하는 방법이 있었지만 앞으로 사용할 모든 외부 라이브러리가 오픈소스임이 보장되지 않기 때문에 이 문제를 해결해야 했음
> * ABI 문제를 피하기 위한 방법으로 컴파일러와 컴파일러 버전, 옵션 등을 맞춰서 빌드하는 방법이 있지만 다양한 외부 라이브러리를 사용할 경우 이 방법은 매우 어려운 방법이 됨
> * 따라서 ABI에 대한 표준을 정의한 C 타입을 사용하도록 component 라이브러리들을 수정
> #### Redis의 "KEYS"를 사용하지 않고 redirect key를 사용
> * Redis에서 "KEYS" 명령어는 레디스의 모든 명령을 멈추고 키 값을 찾을 때까지 탐색(full scan)을 수행하기 때문에 매우 위험한 명령어
> * 따라서 session id - session fields 형태의 hash 타입을 사용하여 유저 세션을 저장할 경우 session id가 있어야 "KEYS"를 사용하지 않고 탐색이 가능
> * 하지만 특정 유저의 session id가 아닌 다른 key로 세션을 검색하는 일이 종종 발생
>   + 예를 들어 계정 식별자(account id)나 닉네임 등
> * 이런 경우 필요한 키마다 복사본의 세션들을 유지할 수 없으므로 원본 세션은 1개로 유지하되 다른 키들은 값으로 session id를 갖도록 구성 (redirect key)
> * 따라서 2번의 키-값 질의로 다중키를 사용하는 효과를 얻음
> * 제한적인 요소로, 가능한 변하지 않는 값들을 redirect key로 사용해야 함
> * 또한 세션이 만료될 경우 redirect key도 만료 시키든 redirect key를 사용하여 질의할 때 예외 처리를 하든 추가 작업이 필요
> * 어쨋든 결과적으로 세션이 아닌 다른 redirect key-value도 lifecycle을 세션과 맞춰 주고 일관성을 맞춰줘야 하는 번거로움이 있음
> #### Redis++ 질의 시 Sync 사용하던 부분을 Async로 전환, 그리고 개선해야 할 일들
> * Redis에 CRUD 작업을 처음부터 Sync 모드로 수행했더니 문제가 생겨서 Async 모드로 변환한 것은 아니고 병목으로 인해 throughput이 나오지 않을 것을 예상하여 개발 도중에 Sync -> Async 모드로 변경
>   + 로비 서버의 모든 스레드가 Redis CRUD 작업을 위해 blocking 되어 있다면 다른 유저의 요청을 처리하는데 지연 발생
> * 따라서 Redis++을 Async 모드로 사용하여 유저 세션에 대한 CRUD를 수행
> * 결과적으로 로비 스레드가 쉬지 않고 유저의 요청을 처리할 수 있는 구조로 변경
> * 하지만 유저가 보낸 1개의 요청이 여러번 CRUD를 수행해야 하거나 여러 CRUD 결과를 조합해야 하는 경우 복잡해지는 문제가 발생
>   + 로비 서버는 최대한 유저 데이터를 들고 있지 않는 stateless를 추구하는데 여러번 차례차례 순서대로 요청하여 처리한다면 상태(context)를 유지해야 하고 이것은 stateless를 위반하게 됨
>   + 따라서 콜백에 콜백에 콜백에.. 콜백 지옥이 펼쳐짐 (마치 예전의 nodejs 콜백 지옥처럼)
>   + 참고로 첫 번째 요청의 콜백에서 그 다음 요청들을 동기로 처리하려는 시도는 해봤으나 Redis++ 비동기 콜백 함수 내에서 시간이 오래 걸리는 blocking(동기 모드) 함수 호출을 금지하고 있어서 불가능했음
>     + 호출해봤는데 데드락처럼 아무 반응이 없었음
> * 결과적으로 Redis++의 Async 모드도 조금 복잡한 처리에 사용하기 썩 좋은 모델은 아닌 것 같고 std::async나 별도 스레드 풀을 사용하여 Sync 모드로 처리하는 구조가 적합해 보임
>   + transaction을 사용하기에 더 적합한 모드도 Sync 모드로 보임
>   + Redis Lua script를 Async 모드에서 사용하는 방법도 고려했지만 Redis Cluster에서는 스크립트에서 접근하는 모든 데이터가 하나의 노드에 있어야 하는 제한이 있음
>     + 물론 hash slot을 사용해서 임의의 데이터셋을 하나의 노드에 몰아 넣고 스크립트를 돌려도 되지만 앞서 설명한 redirect key를 사용할 경우 hash slot을 만들어내기 어려운 상황
>     + 이 부분에서 고민하다가 더 이상 진전하지 못하고 redirect key + 2~3중 콜백을 사용하는 구조에서 머물러 있는 상태
> #### 개발 환경에서의 멀티 & 자동 로그인 지원을 위한 중복 로그인 처리
> * 앞서 인증 서버에서 멀티 클라이언트와 멀티 로그인을 지원하는 부분의 연장선
> * 개발 환경에서 멀티 클라이언트를 실행하여 테스트하는 작업자 입장에서는 클라이언트 하나하나 사용할 계정을 정하고 기억하기가 매우 까다로움
> * 이를 해결하기 위해서 클라이언트가 사용 가능한 로그인 수단(계정 또는 토큰)을 모아서 하나씩 시도해보고 중복 로그인이 발생하면 그 다음 수단으로 로그인을 시도하는 방법으로 로그인 시도
> * 물론 초창기에는 클라이언트를 실행하는 호스트의 호스트네임에 postfix를 붙여가며 인증 없이 자동으로 로그인 처리를 했으나 호스트네임만으로 사용자 구분이 어려워 계정 시스템을 도입
> * 개발 환경에서는 인증 없이 로그인이 가능하도록 운영하는게 좋아보이나 인증 기능을 사용하는 로그인 또한 개발 환경에서 테스트를 해봐야 한다는 (예전)TD님의 철학이 반영됨
> * 서비스 환경에서는 중복 로그인 발생 시 클라이언트는 정상적인 중복 로그인 에러를 출력하거나 기존 클라이언트 연결을 끊고 로그인 시도한 클라이언트가 플레이할 수 있도록 구현

</details>

## 1.4 Game
* (서버들의 출시를 위한 scale out이 가능한 구조로의 개선이 지상 과제가 되면서 작성자가 아래 정리한 작업 내용을 작업함)
* (Statistics subsystem은 초기 설계부터 구현 및 운영까지 작성자가 모두 작업함)
### 작업 내용
* 로비 서버와 마찬가지로 자체 세션 서버를 사용한 유저 세션 접근 부분을 memory database component를 로드하여 사용하도록 수정
* 로비 서버와 마찬가지로 게임 서버 서로간의 연결을 제외한 나머지 서버들과의 연결 복잡도를 낮추기 위해 message queue client component를 로드하여 사용하도록 수정
* 게임 서버 상태를 모니터링하기 위해 통계 지표를 처리하는 통계 시스템(Statistics System)이 있으며 게임 서버에서 통계 시스템과 연동하기 위한 부분으로 통계 서브시스템을 구현하여 통계 시스템 연동을 지원
  + 통계 지표로 CPU와 Memory(Virtual/Physical), Game Object Count, FPS, User Count, Game Server Status 등이 있음

&emsp;&emsp;&emsp;![game_server_statistics_3_1](https://github.com/user-attachments/assets/9f82ed7c-a66c-4b4b-9cfb-4f42efb15e74)

### Trouble shooting
<details>
<summary>Trouble shooting 사례 보기</summary>

> #### 자체 서버 프레임워크의 유저 세션과 memory database 유저 세션 간의 충돌
> * C/C++로 작성한 네트워크 + 데이터베이스 + 스케줄러 + 시스템 관련한 자체 서버 프레임워크가 있음
>   + 예전 TD님이 직접 만듦
> * 서버 프레임워크에서 네트워크 관련 부분에 연결(socket connection)과 여러 연결들을 가질 수 있는 별도 세션(channel) 개념이 있음
>   + 서버 프레임워크에서 다중 연결을 단일 세션으로 구성하도록 설계한듯 보이지만 실제로 단일 연결 및 단일 세션으로만 사용 중
> * 세션에 있는 모든 연결이 종료되면 스케줄러에 의해 일정 시간 세션이 유지되다가 삭제됨
>   + 세션이 만료되기 전에 클라이언트가 재접속을 한다면 재사용하(려)는 구조로 되어 있음
>     + 게임 플레이어에 대한 데이터를 연결이 끊긴다고 바로 날리지 않고 별도 GC 역할을 수행하는 곳에 모아두었다가 세션이 삭제되면 날리고 재접속하면 세션과 같이 재사용하는 구조로 되어 있음
>     + 월드(서버)에서 연결이 끊긴 플레이어에 대한 데이터는 월드에 남아있지 않으므로 다른 플레이어와 상호작용을 할 수 없음 (잘 설계한듯)
>   + 하지만 실제로 세션 만료 기간은 거의 0초로 설정되어 있어서 연결 종료 시 세션도 거의 동시에 삭제하는 것으로 세션 개념을 사용하지 않고 있음 (잉?)
> * 이 상황에서 서버의 scale out이 가능한 구조로의 개선을 게임 서버에도 적용하게 되면서 Redis에서 관리하는 세션과 서버 프레임워크에서의 세션이 충돌하게 됨
>   + 세션이 중복으로 존재하는 셈
> * 처음에는 서버 프레임워크에 있는 세션의 만료 기간을 Redis의 세션 만료 기간과 동일하게 설정하거나 더 길게 설정해서 조화시키려고 했음
>   + Redis의 세션이 만료되면 이를 서버가 통지 받고 서버 프레임워크에서의 세션을 삭제하는 방법을 고려하고 있었음
>   + 이 부분에 대한 결정은 컨텐츠 작업하시는 분들과 충분한 논의 없이 진행한터라 그분들도 이 사정을 제대로 알지 못했음 (워낙 정신없이 바쁘셔서..)
> * 하지만 서버 프레임워크의 세션 만료 시간을 늘리자 서버 크래시가 발생
>   + 서버 프레임워크 내부적인 버그로 세션 재사용에 대해 충분한 작업이 되지 않았음
>   + 서버 프레임워크에서 스케줄러에 들어온 task(세션이 근본적으로 task임)를 처리하고나면 무조건 삭제하는 로직이 문제였음
>     + 삭제된 (dangling) 세션을 참조해서 크래시 발생
>     + 무조건 삭제하지 않도록 변경하는 것은 다른 task들이 대부분 이 처리에 의존적이라 쉽게 고칠 수 있는 부분이 아니었음
> * 서버 크래시가 발생한 후 팀장(겸 TD)님이 개입하게 되면서 컨텐츠 작업자와 함께 제대로된 논의를 시작함
>   + 서버 프레임워크에서 사용하는 세션 개념은 더이상 사용하지 않고 연결(connection)과 동일한 개념으로 취급하기로 결정
>   + 따라서 스케줄러에서 세션 정리하는 부분과 플레이어 데이터를 정리하기 위해 모아놓는 곳(GC)의 연결(의존성)을 끊고 Redis 세션과 묶기로 결정
>   + 하지만 당시 우선순위가 더 높은 작업들이 밀려 있었고 플레이어 데이터를 재사용하는 부분은 당장 필요로 하지 않은 터라 일감만 만들고 차후 진행하기로 일정 조율됨
> #### 게임 서버 퍼포먼스 측정 시 통계 시스템을 분석 도구로써 활용
> * 외부 출시(지스타) 등을 앞두고 게임 서버에 밀어 넣은 컨텐츠를 테스트하며 퍼포먼스 측정을 수행
> * 게임 서버에 있는 모든 것들은 게임 오브젝트로 취급되며 스폰(spawn) 시스템에 의해 필요로 하는 다른 오브젝트들이 생성되는 메커니즘
> * 따라서 데이터 파일이나 서버에서 실행할 (스폰)스크립트에 의해 정상적으로 게임 오브젝트들을 생성하고 제거하는지 확인이 필요
> * 게임 서버의 스폰 시스템은 통계 서브시스템과 밀접하게 연관되어 있으며 현재 생성된 게임 오브젝트 개수 등을 통계 시스템에 전달하여 수치와 그래프로 확인이 가능
> * 또한 게임 서버는 월드의 지역을 파티션(shard) 단위로 쪼개서 월드를 진행시킴
>   + 즉, 파티션 단위로 프레임이 돌고 FPS를 측정할 수 있음
> * 서버의 프레임을 통계 시스템에 전달하여 게임 플레이어 증가에 따른 게임 오브젝트 개수와 FPS 추이를 파악할 수 있음
> * 그 외 게임 서버에서 측정이 필요한 지표들은 모두 통계 서브시스템을 통해 통계 시스템에 적재하고 조회해서 퍼포먼스 측정에 사용함
> * 실제 위 지표들을 사용하여 서버 스폰 시스템 버그 파악과 퍼포먼스 측정 사례
>   + 퍼포먼스 테스트 도중 특정 시점에 여러 클라이언트에서 플레이가 원활하지 않았음
>   + 통계 시스템으로 서버 지표를 보니 당시 프레임이 많이 떨어짐
>   + 그 시점에 특정 게임 오브젝트가 무수히 생성됨
>   + 알고보니 임의의 플레이어가 디버그 커맨드로 아직 미완성된 컨텐츠(게임 오브젝트)들을 생성했고 이는 버그가 있었음
>   + 또한 퍼포먼스 테스트가 끝나고 1개 파티션당 최대 몇개까지 플레이어(PC)들과 게임 오브젝트들을 밀어 넣을 수 있는지 등의 구체적인 수치를 알아낼 수 있었음
> * 위 사례들 외에도 통계 시스템을 사용한 모니터링으로 게임 서버의 여러 문제들의 현상 파악과 다른 여러 문제들을 파악하는데 도움이 되는 객관적인 데이터를 제공함

</details>

# 2. Infrastructure
## 2.1 Statistics System
* (통계 시스템 관련 모든 작업은 작성자가 전담함)
### 사용 목적
* 게임 서버 상태를 모니터링하기 위한 지표 수집 및 저장 관리
### 구성 요소
#### Statistics db (InfluxDB)
* 스트림(브랜치) 또는 production 환경마다 전용 bucket(rdb의 database와 대응되는 개념)으로 데이터를 나누어 관리
* InfluxQL 자체 쿼리 언어를 사용하여 게임 관련 지표 조회
#### Viewer (Grafana)
* 개발자들이 게임 서버 상태를 쉽게 모니터링하기 위해 웹 서비스 형태로 서비스 제공
* InfluxQL 언어를 사용하여 지표를 시각화 (그래프)
#### World contents visualize
##### 개요
* 게임 서버의 게임 오브젝트들을 지도(맵) 상에서의 분포를 쉽게 파악하기 위해 자체 개발한 툴
* 통계 db로부터 게임 오브젝트 위치 데이터(지표)를 조회하여 동영상처럼 시간대 별로 맵 뷰어에 출력
* 누구나 쉽게 접근하기 위해 웹으로 서비스 제공
##### Visualizer
* frontend framework인 Vue.js 사용
* 웹 브라우저에서 맵 뷰어로 OpenLayers 사용
* UI 디자인은 Vuetify framework 사용
* 스트리밍 형식으로 데이터를 조회하여 동영상처럼 재생하며 출력
* 맵 뷰어에서 게임 오브젝트 개수를 그리드맵(grid map) 형식으로 나누어 분포도를 지역적으로 한눈에 알 수 있도록 기능 제공
* 실제 시현 영상

&emsp;&emsp;![KakaoTalk_20240827_225506280-ezgif com-video-to-gif-converter](https://github.com/user-attachments/assets/c8a84261-14f8-4986-85c6-926a974f577b)

### Trouble shooting
<details>
<summary>Trouble shooting 사례 보기</summary>

> #### InfluxDB의 OOM에 의한 강제 재시작 현상
> * 
> #### World contents visualizer - snapshot 데이터 vs diff 데이터

</details>

## 2.2 Log System
* (로그 시스템 관련 모든 작업은 작성자가 전담함)
### 사용 목적
* 게임 서버에서 발생한 로그를 통합 저장하고 관리
* 개발자 또는 컨텐츠 기획자 등이 서버에서 발생한 로그를 분석하기 위한 통계 및 시각화 등의 기능 제공
### 구성 요소
#### Log forwarder (Fluentd)
* 게임 서버에서 남긴 로그 파일의 내용을 감시하면서 새로운 로그(행)가 추가되면 이를 추출하여 로그 수집기로 전송
* 로그 파일 이름으로 실행된 서버의 스트림(브랜치), 맵 이름, 버전, 실행 호스트 등을 추출
* 호스트에 직접 로그 forwarder(agent)를 설치하거나 컨테이너화한 후 서버가 실행되고 있는 호스트에 배포하여 서버 로그 전송
#### Log aggregator (Fluentd)
* 여러 로그 전달자로부터 수집한 로그를 파싱하고 분류해서 로그 저장소에 적재
* 로그의 날짜 및 시간, 카테고리, 레벨(debug, info 등), 소스 파일과 행(line) 등을 추출
  + 소스파일 + 행 조합 통계로 불필요한 로그 추적에 용이
  + 뷰어로 로그 조회 시 로그 카테고리나 레벨로 필터링하여 조회 가능
* 컨텐츠적인 로그도 따로 파싱 및 분류하여 통계를 낼 수 있도록 지원
  + 서버의 공간 정보 쿼리 성능 측정을 위해 사용
#### Log storage (Elasticsearch)
* 로그 관리를 위해 스트림 + 유저(호스트네임) + 년월일 단위로 인덱스를 관리
* TeamCity에서 Elasticsearch RESTful API로 접근하여 로그 백업 및 관리
  + 사용자는 최근 1개월 동안 남은 로그를 조회할 수 있도록 Elasticsearch에 1개월의 로그 인덱스만 로드하고 그 이전 로그는 로그 백업 파이프라인에 의해 백업되도록 자동화
* single node로 운영하였으며 cluster 모드로 운영해보진 못함
  + Elastic cloud라고 K8s operator 써서 cluster 배포해보긴 했는데 실제 적용 단계까지는 못해봄
#### Log viewer (Kibana)
* 개인 서버가 아닌 (배포된) 공용 서버의 로그에 다른 개발자들이 쉽게 접근할 수 있도록 웹 서비스 제공
* 로그 저장소에 저장된 로그들을 다양한 방식으로 조회 및 통계 작업이 가능하도록 지원
### Trouble shooting
<details>
<summary>Trouble shooting 사례 보기</summary>

> #### Elasticsearch 노드당 최대 샤드 개수(max_shards_per_node) 이슈

</details>

## 2.3 Monitoring System
* (모니터링 시스템 관련 모든 작업은 작성자가 전담함)
### 사용 목적
* 실에서 운영하는 장비들과 서비스(소프트웨어)들을 모니터링
### 구성 요소
#### Metrics exporter
##### Node exporter (Windows/Linux)
* 호스트 종료 및 리소스(CPU, Memory, Disk) 과용 탐지
  + 특히 각 호스트의 디스크 부족 현상에 의한 불능 상태를 미리 탐지
##### MySQL exporter
* MySQL 메모리 사용량, 클라이언트 연결, slow queries, table locks 등
  + DBA가 주로 이용
##### Elasticsearch exporter
* 클러스터 현황, JVM 메모리 사용량, cpu 및 메모리 사용량 등
#### Metrics db (Prometheus)
* PromQL로 지표를 다양하게 조회하거나 알람 조건을 설정
#### Alerter (AlertManager)
* 장비의 리소스 과용 시 teams 채널에 메일로 알람 전송
#### Metrics viewer (Grafana)
* Prometheus로부터 다양한 지표를 그래프 등으로 시각화하여 출력
* 아래 사진은 여러 호스트에 설치된 Node Exporter의 지표를 모아서 출력하여 호스트 모니터링을 위한 페이지 (빌드 장비 모니터링)

&emsp;&emsp;![host_monitoring](https://github.com/user-attachments/assets/bca9e331-bc2b-4868-88b0-b00ded345c93)

### Trouble shooting

<details>
<summary>Trouble shooting 사례 보기</summary>

> #### 장비 다운이나 리소스 사용량 감시
> * prometheus가 주기적으로 node epoxrter에 접근하여 지표를 수집해가는 구조
> * node exporter가 설치된 장비가 다운될 경우 해당 node exporter에 접근할 수 없게 되고 이 때부터 타이머를 가동
> * 설정된 시간이 지나도록 node exporter에 접근할 수 없게 되면 alert manager를 통해 팀즈로 알람 전송
> * 이를 통해 운영 중인 장비들이 정상 동작하는지 일일이 감시할 필요 없이 알람을 통해 문제가 발생하는 장비들만 모니터링
>   + 또는 Grafana 페이지에서 통합적으로 모니터링 가능
> * 또한 장비의 디스크 사용량도 설정된 사용량(%) 이상에 도달하면 알람을 전송
> * 특히 빌드 장비에서 CI/CD를 위해 주기적인 파일 sync가 필요하며 이로 인해 디스크 사용량이 과용되는 현상을 방지
>   + 클라이언트 리소스 파일 증가와 UE5의 installed 엔진 때문에 이 기능을 유용하게 사용
> * 그 외 로그(EFK) 서버 장비에서 메모리 사용량이 과용됨에 따라 JVM 사용량 증가나 유틸리티 프로그램들의 문제를 예측할 수 있음
> * 또한 게임 서버 퍼포먼스 테스트 시 실행한 장비의 CPU 및 메모리 사용량 또한 측정하여 결과 지표로 사용
</details>

# 3. DevOps
## 3.1 CI
### VCS (Version control system)
* Perforce 사용 (빌드관리팀이 퍼포스 서버를 직접 운영)
* 사용자(개발자나 기획자 등)는 P4V를 사용하여 소스코드 파일이나 데이터 파일, 리소스 파일(아트) 등을 내려 받고 (sync) 추가 및 수정하여 업로드(submit) 수행
### Build & Run
* 크로스 플랫폼(Windows/Linux) 지원을 위해 CMake/Make 사용
* CMake를 사용하여 프로젝트 별로 Visual studio 프로젝트 파일 및 make 파일 생성
  + 윈도우 환경에서 Visual studio를 사용하여 작업 (주로 디버깅 용도로 사용)
  + Visual studio code도 사용 (주로 소스코드 작업할 때 사용)
* Make를 사용하여 빌드
  + Linux 환경은 원래 Make 명령어로 빌드가 가능하지만 윈도우 환경에서는 Visual studio 프로젝트에서 빌드 (또는 msbuild.exe 사용)
  + 빌드를 Make로 통일하기 위해 Make 스크립트를 사용하여 빌드
* 컴파일러는 윈도우 환경에서 MSVC를, 리눅스 환경에서 Clang 사용
* 실행
  + 서버 소스코드를 내려받아서 빌드를 돌리지 않아도 기본적으로 서버 CI에 의해 자동 서밋되는 서버 바이너리들로 서버 실행이 가능
  + 서버 실행에만 필요한 설치 프로그램으로 vc_redist.x64(2022), mysql-odbc-connector(v8.0.28)가 있음
### TeamCity
* Teamcity에서 job(build configuration)으로 서버 별로 빌드 CI를 돌림
  + Teamcity에 VCS로 P4를 지정하고 P4에서 각 서버별 소스 파일들 변경이 생기는지 감시하다가 변경이 발생하면 Teamcity가 트리거링 되어 CI 빌드 수행
  + CI로 빌드한 결과물들을 artifacts에 저장하고 다시 VCS에 submit하여 사용자들이 일일이 서버/클라 빌드를 자리에서 하지 않더라도 최신 버전을 실행할 수 있도록 지원
## 3.2 CD
### Containerize (Docker)
### K8s (On-Premise)
### NCKUBE
## 3.3 GStar
### Ansible
