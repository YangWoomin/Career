# 0. Contents Table
* [1. Server Development](#1-Server-Development)
  + [1.1 Server Architecture](#11-Server-Architecture)
  + [1.2 Auth](#12-Auth)
  + [1.3 Lobby](#13-Lobby)
  + [1.4 Game](#14-Game)
* [2. Infrastructure](#2-Infrastructure)
  + [2.1 Statistics System](#21-Statistics-System)
  + [2.2 Log System](#22-Log-System)
  + [2.3 Monitoring System](#23-Monitoring-System)
* [3. DevOps](#3-DevOps)
  + [3.1 CI](#31-CI)
  + [3.2 CD](#32-CD)

# 1. Server Development
## 1.1 Server Architecture
* 경력 기술서 내용의 이해를 돕기 위한 간단한 서버 구성도
* 참고로 이 경력 기술서에서 등장하는 "월드" 또는 "월드(서버)"의 뜻은 아래에 있는 월드 서버를 의미하지 않고 게임 서버에 의해 구현된 월드 그 자체를 의미함

![server2](https://github.com/user-attachments/assets/9d7fd7c4-c20b-4a51-9c4c-1d51fc2b3576)

## 1.2 Auth
* (인증 서버는 초기 설계부터 구현 및 운영까지 작성자가 모두 작업함)
### 작업 내용
* 사용자가 외부에서 사용하는 플랫폼 계정을 게임 내 계정(account id)과 바인딩하여 외부 플랫폼 계정으로 게임을 플레이할 수 있게 지원
  + 연동한 외부 플랫폼은 사내 계정 관리 시스템(nano)과 스팀이 있음
* 인증 서버로부터 사용자가 인증된 클라이언트에게 인증 서버 외 다른 서버나 외부 시스템에서 쉽게 인증(권한 부여)할 수 있도록 내부 토큰(LLL Token) 시스템 지원
  + 클라이언트는 지급 받은 내부 토큰을 다른 서버(로비)에게 제출하여 간단한 인증 수행
* 내부(자체) 계정 시스템 지원
* 처음에 Golang + Redis로 만들었으나 (auth1) 자체 서버 프레임워크(C/C++) 기반으로 다시 만듦 (auth2)
### Trouble shooting
<details>
<summary>Trouble shooting 사례 보기</summary>

> #### 개발 환경에서의 멀티 클라이언트 및 멀티 로그인 지원을 위한 내부 계정 시스템 도입
> * 클라이언트/서버 개발자들이 하나의 로컬 PC에서 여러 개의 클라이언트를 실행하여 테스트 진행하는 경우가 잦았음
> * 사내 계정(nano)은 1인 1개씩 부여되므로 1개 nano 계정으로 밖에 로그인을 할 수 밖에 없는 상황
> * 하지만 게임 규칙 상 1개 클라이언트는 1개 계정으로만 플레이가 가능하고 서버 기술적으로도 여러개 클라이언트가 1개의 계정을 동시에 사용하는 경우는 지원되지 않았음
> * 따라서 1인당 다수의 계정이 필요한 상황임이 인지되었고 인증 서버를 개발하여 오픈하기 전에 내부 계정 시스템을 도입
> * 인증 db에 nano 계정과 게임 계정(account id)을 바인딩하는 테이블 외에 자체 계정(id/pw)을 저장하는 테이블을 추가
> * (내부)계정 관리 툴(웹 서비스)에 사용자가 직접 nano 계정으로 접근하여 내부 계정을 생성하고 지급 받음
>   + 계정 관리 툴 + 계정 관리 서비스는 별도 담당자가 작업 및 관리
>   + 계정 관리 서비스에서 인증 db에 접근하여 id/pw를 추가하거나 삭제 등의 동작 지원
> * 인증 서버는 OAuth 토큰 기반 외부 IdP(nano 및 스팀) 인증 방법 외에 내부 계정 인증 서비스를 제공
> #### 내부 계정 토큰화 지원
> * 내부 계정은 id/pw 형식이며 클라이언트에서 직접 입력하여 로그인할 수도 있지만 별도 파일에 저장하여 자동 로그인 시에도 사용 가능
>   + 자동 로그인은 사용자가 직접 id/pw를 입력할 필요 없이 클라이언트가 계정이 저장된 파일로부터 계정을 읽어들여 로그인하는 기능
> * 하지만 이로 인해 계정이 평문 그대로 파일에 노출되는 것과 id/pw를 한번 발급하면 관리(특히 회수/무효화)가 어렵다는 문제가 생김
> * 따라서 평문 노출을 없애고 발급한 계정의 만료 기간을 설정하기 위해 내부 토큰화 기능을 지원
>   + 사용자는 계정 관리 툴에서 계정을 생성한 후 토큰화를 추가로 수행하여 토큰이 저장된 파일을 로컬 PC에 다운로드
>     + id/pw를 토큰 생성 시 토큰 필드에 추가하여 encryption
>   + 클라이언트가 자동 로그인 시 다운로드한 파일로부터 토큰을 읽어들여 로그인 시도
>   + 계정 관리 서버(python flask)에서 토큰화는 인증 서버가 사용하는 토큰 라이브러리(dll)를 로드하여 토큰을 생성하고 발급
>   + 토큰은 만료시점 필드가 내부에 저장되어 있으며 인증 서버가 토큰 해석 시 토큰이 만료되었으면 로그인을 거절
> #### 내부 토큰 암호화 알고리즘의 AES 256 CBC -> AES 256 ECB 전환
> * OAuth 토큰 해석법으로, 토큰 발급자에게 토큰을 전달하여 해당 토큰의 정보를 질의하거나 토큰을 직접 해석하는 방법이 있음
> * 인증 서버는 기본적으로 토큰을 직접 해석하는 방법을 지원
> * 따라서 토큰을 생성하고 해석하는 별도 라이브러리(dll, token)를 토큰 관련 작업이 필요한 서버들이 로드하여 토큰을 생성하고 암호화하거나 복호화 수행
> * 토큰은 JWT 포맷을 따르며 암호화 알고리즘으로 AES 256 ECB를 사용
>   + cbc(cipher block chaining)를 사용하지 않고 ecb(electronic code book)를 사용한 이유는 cbc 특성상 이전에 암호화된 블럭을 (다시 XOR하여) IV로 사용하기 때문에 토큰간 의존성(순서)이 생김
>   + 따라서 스트림 데이터에 쓰이는 더욱 강력한 암호화 알고리즘인 cbc를 사용하는 대신 조금 덜 보안적이지만 의존성을 없애기 위해 ecb 사용
>   + 토큰 질의(introspection) 서비스는 외부 시스템에서 사용할 수 있게 제공할 예정이었으나 이 부분 작업 진행은 우선순위 이슈로 홀드된 상태
> * JWT 토큰 암호화는 HS256 사용
> * 토큰 암호화 키와 HMAC 키는 별개이며 모두 소스코드에 박혀져 있음
>   + 소스코드 접근은 일부 인원에게만 허용됨
> * 토큰 암호화/복호화를 위해 openssl3 라이브러리 사용
> #### 토큰 vs 세션
> * LLL 시스템은 토큰과 세션 모두를 채용
>   + 보통 토큰과 세션은 대조되는 개념으로 취급됨
>   + 사용자 인증 관련한 부분은 토큰을 사용하고 내부 시스템에서 좀 더 복잡한 유저의 상태 관리는 세션을 사용

</details>

## 1.3 Lobby
* (서버들의 출시를 위한 scale out이 가능한 구조로의 개선이 지상 과제가 되면서 작성자가 아래 정리한 작업 내용을 작업함)
### 작업 내용
#### Login
* 로비 서버가 자체 세션 서버(C/C++)와 연동하여 유저 세션을 관리하던 부분을 Redis++을 사용하여 Redis와 연동하도록 수정
  + Redis++을 wrapping한 별도 라이브러리(dll, memory database component)를 구현 후 로비 서버에서 로드하여 사용
* 위 작업으로 인한 유저 로그인 처리 로직 대부분을 수정
#### Nickname & Character
* 인게임에서 사용할 닉네임과 캐릭터 관련 준비 작업(In-game Preparation)하는 부분이 원래 로비 서버가 별도 서버들과 연동하여 수행했던 부분들을 모두 라이브러리화하여 연동하도록 수정
  + 로비에서 닉네임 생성/수정 또는 캐릭터 생성/삭제 등의 작업이 가능함
  + 계정 서버와 캐릭터 서버가 별도로 존재했었음 (당시 서버 구조가 MSA를 추구)
  + 별도 서버들을 모두 라이브러리(dll)로 변환하여 로비 서버에서 로드하고 사용하도록 수정
#### Inter-server Communication 
* 서버들 사이의 연결 복잡도를 줄이고 통신 방법을 표준화하기 위하여 메시지 큐(Kafka) 도입
  + 근실시간 동기화 수준 정도면 충족되는 서버들 사이의 연결에 한함
  + memory database component와 마찬가지로 librdkafka 라이브러리를 wrapping하여 별도 라이브러리(dll, message queue client component)를 구현 후 로비 서버에서 로드하여 사용
### Trouble shooting
<details>
<summary>Trouble shooting 사례 보기</summary>
  
> #### Component dll 로드 시 ABI 문제
> * 앞서 설명한 memory database component, message queue client component 등 오픈소스 라이브러리를 한번 wrapping하여 다시 dll로 뽑아냄
>   + 오픈소스의 필요한 기능만 제한적으로 사용하도록, 일관적인 코드 사용 등을 위해 wrapping함
> * 서버 빌드 시 컴파일러 버전과 옵션 등의 세팅을 모든 개발자 PC에서 동일하게 사용하도록 빌드 환경 및 시스템이 구축되어 있어서 C++ 요소들을 그대로 dll export하여도 문제가 되지 않았음
>   + 위에서 설명한 서버 구성도의 모든 서버들은 C/C++로 작성된 공통의 서버 프레임워크(dll)를 사용함
> * 하지만 vcpkg로 다운받은 redis++ 라이브러리나 librdkafka 라이브러리를 wrapping한 component 라이브러리를 서버에서 로드하여 사용할 때 ABI 문제가 발생
>   + component 라이브러리의 소스코드가 vcpkg 라이브러리 함수 호출 후 반환값(문자열 등)에 접근하면 크래시 발생
>   + 오픈소스라서 직접 소스코드를 다운받고 빌드하는 방법이 있었지만 앞으로 사용할 모든 외부 라이브러리가 오픈소스임이 보장되지 않기 때문에 이 문제를 해결해야 했음
> * ABI 문제를 피하기 위한 방법으로 컴파일러와 컴파일러 버전, 옵션 등을 맞춰서 빌드하는 방법이 있지만 다양한 외부 라이브러리를 사용할 경우 이 방법은 매우 어려운 방법이 됨
> * 따라서 ABI에 대한 표준을 정의한 C 타입을 사용하도록 component 라이브러리들을 수정
> #### Redis의 "KEYS"를 사용하지 않고 secondary index를 사용
> * Redis에서 "KEYS" 명령어는 레디스의 모든 명령을 멈추고 키 값을 찾을 때까지 탐색(full scan)을 수행하기 때문에 매우 위험한 명령어
> * 따라서 session id - session fields 형태의 hash 타입을 사용하여 유저 세션을 저장할 경우 session id가 있어야 "KEYS"를 사용하지 않고 탐색이 가능
> * 하지만 특정 유저의 session id가 아닌 다른 key로 세션을 검색하는 일이 종종 발생
>   + 예를 들어 계정 식별자(account id)나 닉네임 등
> * 이런 경우 필요한 키마다 복사본의 세션들을 유지할 수 없으므로 원본 세션은 1개로 유지하되 다른 키들은 값으로 session id를 갖도록 구성 (secondary index)
> * 따라서 2번의 키-값 질의로 다중키를 사용하는 효과를 얻음
> * 제한적인 요소로, 가능한 변하지 않는 값들을 secondary index로 사용해야 함
> * 또한 세션이 만료될 경우 secondary index도 만료 시키든 secondary index를 사용하여 질의할 때 예외 처리를 하든 추가 작업이 필요
> * 어쨋든 결과적으로 세션이 아닌 다른 secondary index의 key-value도 lifecycle을 세션과 맞춰 주고 일관성을 맞춰줘야 하는 번거로움이 있음
> #### Redis++ 질의 시 Sync 사용하던 부분을 Async로 전환, 그리고 개선해야 할 일들
> * Redis에 CRUD 작업을 처음부터 Sync 모드로 수행했더니 문제가 생겨서 Async 모드로 변환한 것은 아니고 병목으로 인해 throughput이 나오지 않을 것을 예상하여 개발 도중에 Sync -> Async 모드로 변경
>   + 로비 서버의 모든 스레드가 Redis CRUD 작업을 위해 blocking 되어 있다면 다른 유저의 요청을 처리하는데 지연 발생
> * 따라서 Redis++을 Async 모드로 사용하여 유저 세션에 대한 CRUD를 수행
> * 결과적으로 서버 스레드가 쉬지 않고 유저의 요청을 처리할 수 있는 구조로 변경
> * 하지만 유저가 보낸 1개의 요청이 여러번 CRUD를 수행해야 하거나 여러 CRUD 결과를 조합해야 하는 경우 복잡해지는 문제가 발생
>   + 로비 서버는 최대한 유저 데이터를 들고 있지 않는 stateless를 추구하는데 여러번 차례차례 순서대로 요청하여 처리한다면 상태(context)를 유지해야 하고 이것은 stateless를 위반하게 됨
>   + 따라서 콜백에 콜백에 콜백에.. 콜백 지옥이 펼쳐짐 (마치 예전의 nodejs 콜백 지옥처럼)
>   + 참고로 첫 번째 요청의 콜백에서 그 다음 요청들을 동기로 처리하려는 시도는 해봤으나 Redis++ 비동기 콜백 함수 내에서 시간이 오래 걸리는 blocking(동기 모드) 함수 호출을 금지하고 있어서 불가능했음
>     + 호출해봤는데 데드락처럼 아무 반응이 없었음
> * 결과적으로 Redis++의 Async 모드도 조금 복잡한 처리에 사용하기 썩 좋은 모델은 아닌 것 같고 std::async나 별도 스레드 풀을 사용하여 Sync 모드로 처리하는 구조가 적합해 보임
>   + transaction을 사용하기에 더 적합한 모드도 Sync 모드로 보임
>   + Redis Lua script를 Async 모드에서 사용하는 방법도 고려했지만 Redis Cluster에서는 스크립트에서 접근하는 모든 데이터가 하나의 노드에 있어야 하는 제한이 있음
>     + 물론 hash slot을 사용해서 임의의 데이터셋을 하나의 노드에 몰아 넣고 스크립트를 돌려도 되지만 앞서 설명한 secondary index를 사용할 경우 hash slot을 만들어내기 어려운 상황
>   + Redis++을 Async 모드로 최대 3중 콜백까지 사용하여 필요한 기능들을 일반화했고 이대로 사용중
> #### 개발 환경에서의 멀티 & 자동 로그인 지원을 위한 중복 로그인 처리
> * 앞서 인증 서버에서 멀티 클라이언트와 멀티 로그인을 지원하는 부분의 연장선
> * 개발 환경에서 멀티 클라이언트를 실행하여 테스트하는 작업자 입장에서는 클라이언트 하나하나 사용할 계정을 정하고 기억하기가 매우 까다로움
> * 이를 해결하기 위해서 클라이언트가 사용 가능한 로그인 수단(계정 또는 토큰)을 모아서 하나씩 시도해보고 중복 로그인이 발생하면 그 다음 수단으로 로그인을 시도하는 방법으로 로그인 시도
> * 물론 초창기에는 클라이언트를 실행하는 호스트의 호스트네임에 postfix를 붙여가며 인증 없이 자동으로 로그인 처리를 했으나 호스트네임만으로 사용자 구분이 어려워 계정 시스템을 도입
> * 개발 환경에서는 인증 없이 로그인이 가능하도록 운영하는게 좋아보이나 인증 기능을 사용하는 로그인 또한 개발 환경에서 테스트를 해봐야 한다는 (예전)TD님의 철학이 반영됨
> * 서비스 환경에서는 중복 로그인 발생 시 클라이언트는 정상적인 중복 로그인 에러를 출력하거나 기존 클라이언트 연결을 끊고 로그인 시도한 클라이언트가 플레이할 수 있도록 구현

</details>

## 1.4 Game
* (서버들의 출시를 위한 scale out이 가능한 구조로의 개선이 지상 과제가 되면서 작성자가 아래 정리한 작업 내용을 작업함)
* (Statistics subsystem은 초기 설계부터 구현 및 운영까지 작성자가 모두 작업함)
### 작업 내용
* 로비 서버와 마찬가지로 자체 세션 서버를 사용한 유저 세션 접근 부분을 memory database component를 로드하여 사용하도록 수정
* 로비 서버와 마찬가지로 게임 서버 서로간의 연결을 제외한 나머지 서버들과의 연결 복잡도를 낮추기 위해 message queue client component를 로드하여 사용하도록 수정
* 게임 서버 상태를 모니터링하기 위해 통계 지표를 처리하는 통계 시스템(Statistics System)이 있으며 게임 서버에서 통계 시스템과 연동하기 위한 부분으로 통계 서브시스템을 구현하여 통계 시스템 연동을 지원
  + 통계 지표로 CPU와 Memory(Virtual/Physical), Game Object Count, FPS, User Count, Game Server Status 등이 있음

&emsp;&emsp;&emsp;![game_server_statistics_3_1](https://github.com/user-attachments/assets/9f82ed7c-a66c-4b4b-9cfb-4f42efb15e74)

### Trouble shooting
<details>
<summary>Trouble shooting 사례 보기</summary>

> #### 자체 서버 프레임워크의 유저 세션과 memory database 유저 세션 간의 충돌
> * C/C++로 작성한 네트워크 + 데이터베이스 + 스케줄러 + 시스템 관련한 자체 서버 프레임워크가 있음
>   + 예전 TD님이 직접 만듦
> * 서버 프레임워크에서 네트워크 관련 부분에 연결(socket connection)과 여러 연결들을 가질 수 있는 별도 세션(channel) 개념이 있음
>   + 서버 프레임워크에서 다중 연결을 단일 세션으로 구성하도록 설계한듯 보이지만 실제로 단일 연결 및 단일 세션으로만 사용 중
> * 세션에 있는 모든 연결이 종료되면 스케줄러에 의해 일정 시간 세션이 유지되다가 삭제됨
>   + 세션이 만료되기 전에 클라이언트가 재접속을 한다면 재사용하(려)는 구조로 되어 있음
>     + 게임 플레이어에 대한 데이터를 연결이 끊긴다고 바로 날리지 않고 별도 GC 역할을 수행하는 곳에 모아두었다가 세션이 삭제되면 날리고 재접속하면 세션과 같이 재사용하는 구조로 되어 있음
>     + 월드(서버)에서 연결이 끊긴 플레이어에 대한 데이터는 월드에 남아있지 않으므로 다른 플레이어와 상호작용을 할 수 없음 (잘 설계한듯)
>   + 하지만 실제로 세션 만료 기간은 거의 0초로 설정되어 있어서 연결 종료 시 세션도 거의 동시에 삭제하는 것으로 세션 개념을 사용하지 않고 있음 (잉?)
> * 이 상황에서 서버의 scale out이 가능한 구조로의 개선을 게임 서버에도 적용하게 되면서 Redis에서 관리하는 세션과 서버 프레임워크에서의 세션이 충돌하게 됨
>   + 세션이 중복으로 존재하는 셈
> * 처음에는 서버 프레임워크에 있는 세션의 만료 기간을 Redis의 세션 만료 기간과 동일하게 설정하거나 더 길게 설정해서 조화시키려고 했음
>   + Redis의 세션이 만료되면 이를 서버가 통지 받고 서버 프레임워크에서의 세션을 삭제하는 방법을 고려하고 있었음
>   + 이 부분에 대한 결정은 컨텐츠 작업하시는 분들과 충분한 논의 없이 진행한터라 그분들도 이 사정을 제대로 알지 못했음 (워낙 정신없이 바쁘셔서..)
> * 하지만 서버 프레임워크의 세션 만료 시간을 늘리자 서버 크래시가 발생
>   + 서버 프레임워크 내부적인 버그로 세션 재사용에 대해 충분한 작업이 되지 않았음
>   + 서버 프레임워크에서 스케줄러에 들어온 task(세션이 근본적으로 task임)를 처리하고나면 무조건 삭제하는 로직이 문제였음
>     + 삭제된 (dangling) 세션을 참조해서 크래시 발생
>     + 무조건 삭제하지 않도록 변경하는 것은 다른 task들이 대부분 이 처리에 의존적이라 쉽게 고칠 수 있는 부분이 아니었음
> * 서버 크래시가 발생한 후 팀장(겸 TD)님이 개입하게 되면서 컨텐츠 작업자와 함께 제대로된 논의를 시작함
>   + 서버 프레임워크에서 사용하는 세션 개념은 더이상 사용하지 않고 연결(connection)과 동일한 개념으로 취급하기로 결정
>   + 따라서 스케줄러에서 세션 정리하는 부분과 플레이어 데이터를 정리하기 위해 모아놓는 곳(GC)의 연결(의존성)을 끊고 Redis 세션과 묶기로 결정
>   + 하지만 당시 우선순위가 더 높은 작업들이 밀려 있었고 플레이어 데이터를 재사용하는 부분은 당장 필요로 하지 않은 터라 일감만 만들고 차후 진행하기로 일정 조율됨
> #### 게임 서버 퍼포먼스 측정 시 통계 시스템을 분석 도구로써 활용
> * 외부 출시(지스타) 등을 앞두고 게임 서버에 밀어 넣은 컨텐츠를 테스트하며 퍼포먼스 측정을 수행
> * 게임 서버에 있는 모든 것들은 게임 오브젝트로 취급되며 스폰(spawn) 시스템에 의해 필요로 하는 다른 오브젝트들이 생성되는 메커니즘
> * 따라서 데이터 파일이나 서버에서 실행할 (스폰)스크립트에 의해 정상적으로 게임 오브젝트들을 생성하고 제거하는지 확인이 필요
> * 게임 서버의 스폰 시스템은 통계 서브시스템과 밀접하게 연관되어 있으며 현재 생성된 게임 오브젝트 개수 등을 통계 시스템에 전달하여 수치와 그래프로 확인이 가능
> * 또한 게임 서버는 월드의 지역을 파티션(shard) 단위로 쪼개서 월드를 진행시킴
>   + 즉, 파티션 단위로 프레임이 돌고 FPS를 측정할 수 있음
> * 서버의 프레임을 통계 시스템에 전달하여 게임 플레이어 증가에 따른 게임 오브젝트 개수와 FPS 추이를 파악할 수 있음
> * 그 외 게임 서버에서 측정이 필요한 지표들은 모두 통계 서브시스템을 통해 통계 시스템에 적재하고 조회해서 퍼포먼스 측정에 사용함
> * 실제 위 지표들을 사용하여 서버 스폰 시스템 버그 파악과 퍼포먼스 측정 사례
>   + 퍼포먼스 테스트 도중 특정 시점에 여러 클라이언트에서 플레이가 원활하지 않았음
>   + 통계 시스템으로 서버 지표를 보니 당시 프레임이 많이 떨어짐
>   + 그 시점에 특정 게임 오브젝트가 무수히 생성됨
>   + 알고보니 임의의 플레이어가 디버그 커맨드로 아직 미완성된 컨텐츠(게임 오브젝트)들을 생성했고 이는 버그가 있었음
>   + 또한 퍼포먼스 테스트가 끝나고 1개 파티션당 최대 몇개까지 플레이어(PC)들과 게임 오브젝트들을 밀어 넣을 수 있는지 등의 구체적인 수치를 알아낼 수 있었음
> * 위 사례들 외에도 통계 시스템을 사용한 모니터링으로 게임 서버의 여러 문제들의 현상 파악과 다른 여러 문제들을 파악하는데 도움이 되는 객관적인 데이터를 제공함

</details>

# 2. Infrastructure
* (인프라 관련 모든 작업은 작성자가 전담함)
## 2.1 Statistics System
### 사용 목적
* 게임 서버 상태를 모니터링하기 위한 지표 수집 및 저장 관리
### 구성 요소
#### Statistics db (InfluxDB)
* 스트림(브랜치) 또는 production 환경마다 전용 bucket(rdb의 database와 대응되는 개념)으로 데이터를 나누어 관리
* InfluxQL 자체 쿼리 언어를 사용하여 게임 관련 지표 조회
#### Viewer (Grafana)
* 개발자들이 게임 서버 상태를 쉽게 모니터링하기 위해 웹 서비스 형태로 서비스 제공
* InfluxQL 언어를 사용하여 지표를 시각화 (그래프)
#### World contents visualize
##### 개요
* 게임 서버의 게임 오브젝트들을 지도(맵) 상에서의 분포를 쉽게 파악하기 위해 자체 개발한 툴
* 통계 db로부터 게임 오브젝트 위치 데이터(지표)를 조회하여 동영상처럼 시간대 별로 맵 뷰어에 출력
* 누구나 쉽게 접근하기 위해 웹으로 서비스 제공
##### Visualizer
* frontend framework인 Vue.js 사용
* 웹 브라우저에서 맵 뷰어로 OpenLayers 사용
* UI 디자인은 Vuetify framework 사용
* 스트리밍 형식으로 데이터를 조회하여 동영상처럼 재생하며 출력
* 맵 뷰어에서 게임 오브젝트 개수를 그리드맵(grid map) 형식으로 나누어 분포도를 지역적으로 한눈에 알 수 있도록 기능 제공
* 실제 시현 영상

&emsp;&emsp;![KakaoTalk_20240827_225506280-ezgif com-video-to-gif-converter](https://github.com/user-attachments/assets/c8a84261-14f8-4986-85c6-926a974f577b)

&emsp;&emsp;(원본 영상 : https://github.com/user-attachments/assets/7e5cd55c-d74c-4e28-be63-90c6272efe98)

### Trouble shooting
<details>
<summary>Trouble shooting 사례 보기</summary>

> #### InfluxDB의 OOM에 의한 강제 재시작 현상
> * InfluxDB는 내부적으로 일정 기간의 최신 데이터를 메모리(캐시)에 로드해 놓고 있으며 쿼리 시 먼저 메모리에 반영(write)하거나 메모리에 있는 데이터를 전달(read)
> * InfluxDB가 메모리를 무한정 사용할 수 없기 때문에 주기적으로 과거의 데이터를 TSM(Time-Structured Merge Tree) 파일로 flush를 수행
> * TSM 파일들은 효율적인 저장공간 사용(데이터 중복 제거)과 최적화된 읽기를 위해 여러 파일들을 압축(compaction)하여 저장
> * InfluxDB에게 과거 데이터 조회를 요청 시 압축한 TSM 파일을 메모리에 다시 로드하여 압축 해제한 결과를 전달
> * 초창기 InfluxDB를 도입했을 당시 데이터(bucket)에 대한 retention policy는 제한이 없었음 (과거 데이터 삭제를 안했음)
> * 그리고 InfluxDB를 운용하는 장비는 CPU 4 Core에 8GB 램을 사용하는 저사양 장비였음
> * 서버팀 일부 인원이 게임 서버의 과거와 현재의 차이점을 지표로 비교할 일이 생겨서 데이터 조회 범위를 넓은 범위로 지정하여 쿼리함
> * InfluxDB는 도커 컨테이너 위에서 실행되고 있었고 장비의 메모리를 모두 소진하는 상태로 몇분간 지속되다가 OOM에 의해 컨테이너 강제 재시작
>   + 컨테이너의 자원 사용 제한은 없었음
> * 이로 인해 정상적으로 통계 시스템을 사용하던 다른 사용자들이 불편을 겪음
> * 당시 InfluxDB를 클러스터 모드로 사용할 재정적인 여유와 데이터 마이그레이션 파이프라인 구축 등을 수행할 시간이 없었어서 일단 retention policy 기간을 협의하여 1개월로 결정하고 장비 스펙을 업그레이드 함
>   + InfluxDB의 클러스터 모드는 Enterprise 버전에서만 사용 가능했고 이는 유료 버전
> * 위 이슈 이후 메모리 과용으로 InfluxDB가 죽는 일은 없었지만 간단하고 단기적인 방법인 스케일 업 말고 좀 더 장기적이고 유연한 다른 방법을 모색해보지 못한 부분이 아쉬움
> #### World contents visualizer - diff 데이터에서 snapshot 데이터로 변환
> * 앞서 설명한 통계 서브시스템을 통해 게임 서버가 통계 지표를 통계 시스템에 적재
> * 통계 지표 중 visualizer에서 출력하기 위한 지표가 Game Object Location이 있음
>   + Game Object ID, X, Y, Z, tag... 등의 필드로 구성
> * 초창기 개발할 때 이 지표를 게임 서버에서 월드에 있는 모든 게임 오브젝트의 위치를 매 시점(visualizer 영상 프레임)마다 통계 시스템에 적재함 (snapshot 데이터)
>   + 예를 들어 주기가 5초면 5초마다 월드에 있는 모든 게임 오브젝트를 순회하여 좌표를 알아낸 후 적재
>   + 하지만 이 작업을 게임 로직 스레드가 수행했으며 통계 처리를 위한 작업이 게임 로직 처리하는 작업보다 더 오래 걸리게 되는 문제가 발생
>   + 위 문제를 해결하기 위해 통계 서브시스템을 개선
>     + 통계 서브시스템 내부에서 지표 종류 별로 데이터를 보관하도록 하며 별도 스레드가 주기적으로 통계 시스템에 적재하는 구조로 개선
>     + 게임 메인 스레드는 게임 오브젝트의 생성(스폰)/제거(디스폰), 위치 변화가 있을 때만 통계 서브시스템에게 지표를 전달 (diff 데이터)
>     + 통계 서브시스템은 전달받은 지표로 기존 지표를 업데이트 후 통계 시스템에 적재
>   + 위 방법으로 게임 메인 스레드의 통계 관련 작업은 미미할 정도로 줄어들게 됨
>   + 또한 게임 서버에서 통계 시스템으로의 데이터 전송량이 매우 줄어들게 됨
> * visualizer에서 통계 시스템에 있는 게임 오브젝트 지표를 조회하여 맵뷰에 출력
>   + 처음 실 내에 visualizer를 오픈했을 때 당시 visualizer가 조회한 게임 오브젝트 지표를 가지고 영상처럼 재생시키기 위해 조회한 시간 범위에 대해 모든 프레임을 계산
>     + 예를 들어 조회 시간 범위가 10분이고 영상 재생 프레임 단위가 1초면 600 프레임(초)을 계산
>     + 즉, 이 작업은 diff 데이터를 snapshot 데이터로의 변환 작업을 의미
>   + 얼마간 사용하면서 사용자들의 피드백을 받아보니 아래 두가지 문제점이 있었음
>     + 조회 시간 범위를 좀 넓게 하고 게임 오브젝트 개수가 좀 많아지면 프레임 계산하는 시간이 너무 오래 걸림
>     + 조회 시간 시작 시점을 무조건 서버가 실행된 시점으로 해야 하는 강제성
>       + 왜냐하면 특정 시점의 프레임을 계산하려면 서버의 실행 시점부터 순차적으로 계산해야 하기 때문
>       + 특정 시점 이후부터 조회하여 계산을 하면 그 시점 이전에 생성된 게임 오브젝트가 그 시점 이후에 움직임이 없었을 때 그 게임 오브젝트의 데이터가 없음
>       + 삭제도 위와 마찬가지로 존재하지 않던 게임 오브젝트가 갑자기 삭제되는 현상이 발생함
>   + 위 문제들을 해결하기 위해 가장 먼저 떠오른 해결책으로 애초에 게임 서버에서 앞서 설명한 방식인 diff 데이터를 적재하지 않고 snapshot 데이터를 적재
>     + 프레임 계산을 할 필요가 없게끔 게임 서버가 처음부터 프레임 단위로 모든 게임 오브젝트 데이터를 통계 서브 시스템에게 전달하여 적재
>     + 이것만 된다면 visualizer에서는 프레임 계산이 필요 없게 되고 어느 시점이든 어느 범위든 자유롭게 조회하여 출력이 가능함
>     + 하지만 게임 서버에서 월드에 있는 모든 게임 오브젝트를 순회하여 적재하는 방법은 맨 처음 문제와 같이 게임 로직 처리할 시간을 뺏기는 부담이 존재
>       + 정적인(생성된 후 움직임이 없는) 게임 오브젝트는 데이터 파일로 미리 뽑아서 써서 통계 시스템을 태우지 말고 계속 움직이는 게임 오브젝트들만 통계 시스템을 태울까 하는 방법도 고려됨
>       + 하지만 위 방법은 단순히 일부 정적 오브젝트만 걷어내는 효과가 있고 실제로 움직임이 거의 없거나 자주 움직이지 않는 오브젝트들은 여전히 많아서 문제
>         + 특히 NPC(몬스터)들은 정찰을 하는 경우도 있지만 월드에 처음 생성된 자리에 있다가 어그로가 생겨야 움직이는 경우가 태반
>         + 정찰을 하더라도 계속 움직이는 것이 아니고 정찰 포인트마다 한동안 멈췄다가 이동하는 경우도 많음
>   + 게임 서버가 snapshot 데이터를 남기는 것은 비효율적이므로 diff 데이터만 적재하도록 유지
>   + visualizer는 여전히 snapshot 데이터가 필요한 상황
> * 통계 시스템(내부의 통계 db인 InfluxDB)에서 diff -> snapshot 데이터 자동 변환 파이프라인을 추가
>   + InfluxDB 내부 기능으로 task를 주기적으로 실행할 수 있는 기능이 있음
>   + 이 task 기능을 사용하여 저번 task 때 마지막으로 생성한 snapshot 데이터 기준으로 현재 task가 실행된 최근 시점까지의 snapshot 데이터들을 계산
>   + task를 주기적으로 계속 반복하여 모든 diff 데이터를 snapshot 데이터로 변환
>   + 서버가 재시작 된다면 작업하던 snapshot 데이터는 날려서 초기화
>   + visualizer는 통계 시스템에서 적재한 snapshot 데이터를 조회하기 때문에 diff -> snapshot 데이터 변환 작업 필요 없이 자유롭게 지표 조회가 가능

</details>

## 2.2 Log System
### 사용 목적
* 게임 서버에서 발생한 로그를 통합 저장하고 관리
* 개발자 또는 컨텐츠 기획자 등이 서버에서 발생한 로그를 분석하기 위한 통계 및 시각화 등의 기능 제공
### 구성 요소
#### Log forwarder (Fluentd)
* 게임 서버에서 남긴 로그 파일의 내용을 감시하면서 새로운 로그(행)가 추가되면 이를 추출하여 로그 수집기로 전송
* 로그 파일 이름으로 실행된 서버의 스트림(브랜치), 맵 이름, 버전, 실행 호스트 등을 추출
* 호스트에 직접 로그 forwarder(agent)를 설치하거나 컨테이너화한 후 서버가 실행되고 있는 호스트에 배포하여 서버 로그 전송
#### Log aggregator (Fluentd)
* 여러 로그 전달자로부터 수집한 로그를 파싱하고 분류해서 로그 저장소에 적재
* 로그의 날짜 및 시간, 카테고리, 레벨(debug, info 등), 소스 파일과 행(line) 등을 추출
  + 소스파일 + 행 조합 통계로 불필요한 로그 추적에 용이
  + 뷰어로 로그 조회 시 로그 카테고리나 레벨로 필터링하여 조회 가능
* 컨텐츠적인 로그도 따로 파싱 및 분류하여 통계를 낼 수 있도록 지원
  + 서버의 공간 정보 쿼리 성능 측정을 위해 사용
#### Log storage (Elasticsearch)
* 로그 관리를 위해 스트림 + 유저(호스트네임) + 년월일 단위로 인덱스를 관리
* TeamCity에서 Elasticsearch RESTful API로 접근하여 로그 백업 및 관리
  + 사용자는 최근 1개월 동안 남은 로그를 조회할 수 있도록 Elasticsearch에 1개월의 로그 인덱스만 로드하고 그 이전 로그는 로그 백업 파이프라인에 의해 백업되도록 자동화
* single node로 운영하였으며 cluster 모드로 운영해보진 못함
  + Elastic cloud라고 K8s operator 써서 cluster 배포해보긴 했는데 실제 적용 단계까지는 못해봄
#### Log viewer (Kibana)
* 개인 서버가 아닌 (배포된) 공용 서버의 로그에 다른 개발자들이 쉽게 접근할 수 있도록 웹 서비스 제공
* 로그 저장소에 저장된 로그들을 다양한 방식으로 조회 및 통계 작업이 가능하도록 지원
### Trouble shooting
<details>
<summary>Trouble shooting 사례 보기</summary>

> #### Elasticsearch 노드당 최대 샤드 개수(max_shards_per_node) 이슈
> * 기본적으로 Elasticsearch에서 로그는 인덱스라는 논리적인 그룹으로 묶어서 관리
>   + 로그를 삭제하거나 백업(snapshot)하는 자동화 관리 면에서의 단위가 주로 인덱스 단위로 수행
> * 인덱스는 실제로 물리적으로 저장될 때 샤드 단위로 여러 노드에 분산되어 저장됨
>   + 노드는 Elasticsearch Cluster를 이루는 노드를 의미
> * 이 때 노드당 최대 샤드 개수를 기본값 1000개로 제한하고 있음
> * 따라서 로그 인덱스 개수가 늘어나면 샤드 개수도 늘어나고 제한값에 가까워지게 됨
>   + 간단하게 로그 그룹인 인덱스 개수 자체를 줄여서 회피할 수 있음
>   + 하지만 로그 조회나 백업, 삭제 등을 원활하게 수행하기 위해 로그 그룹화는 필수이며, 그룹화를 적당히 세분화하면 로그 유지 기간이 늘어나면서 인덱스도 자연스럽게 늘어나게 됨
>   + 스트림(브랜치), 호스트네임(사용자), 로그 발생 날짜(년월일) 단위로 인덱스를 생성
> * 로그가 무한정 늘어나지 않도록 curator라는 툴로 Elasticsearch의 로그를 최근 1년만 보관하고 그 이전 로그는 삭제하도록 자동화 해놓음
> * 스트림을 3~4개 운영할 때까지는 문제없이 잘 사용했으나 사장님 보고 등의 이슈로 스트림이 더 늘어나자 max_shards_per_node 제한이 발생
>   + 로그를 적재할 때 인덱스 규칙에 따라 인덱스가 없으면 자동으로 생성됨
>   + 이 때 노드에 저장된 인덱스 개수가 max_shards_per_node 설정을 초과할 경우 에러를 출력하고 적재되지 않음
>   + 그리고 Elasticsearch를 클러스터 모드로 사용하지 않고 싱글 노드로 사용하는 상태라서 노드 추가 방법으로 scale out이 불가능한 상태였음
> * 당장 Elasticsearch를 클러스터모드로 전환하기가 어려운 상황이라서 max_shards_per_node 설정값을 증가시켜 문제를 해결
> * 이 해결책은 결국 한계가 있어서 Elasticsearch를 클러스터 모드로 운영하여 확장성있는 해결책으로 전환이 필요하여 전환 시도
> * 처음에는 Elasticsearch를 도커 컨테이너로 여러대 실행하여 클러스터 구성을 시도
> * K8s 환경에서 Elasticsearch를 쉽게 클러스터 모드로 운영할 수 있는, K8s operator 패턴을 적용한 Elastic Cloud를 r&d하여 도입 시도
> * 하지만 K8s 및 Elastic Cloud(또는 클러스터 모드) 도입을 위해 학습해야 하는 비용과 여러 허들을 넘어야 하는 비용이 있고 장비를 추가로 지급 받아야 하는 상황
> * 당장에 max_shards_per_node 설정값을 변경하는 것으로 충분히 운용 가능하여 Elasticsearch 클라우드 전환 보류
>   + 이 부분도 더 진행하지 못해서 아쉬움
> #### Elasticsearch 로그 삭제 및 백업 파이프라인 구축
> * 로그 시스템 도입 후 1년 정도 지났을 때 로그 서버 장비의 디스크 사용량이 90%를 넘었다는 호스트 모니터링 시스템을 통해 알람을 수신
> * Elasticsearch는 데이터 경로의 디스크 사용량이 90% 넘을 경우 인덱스를 읽기 전용으로 잠그며 더이상 로그가 추가되지 않도록 막음 (Elasticsearch 로그에서 확인)
> * 실제로 Elasticsearch의 데이터 경로가 해당 경로의 파일시스템에서 용량의 대부분을 차지
> * 간단하게 Elasticsearch가 보존하는 로그를 최근 1년보다 더 줄여서 보관하도록 curator 설정을 해도 되지만 과거의 로그가 필요할 수 있다는 요구사항이 있었음
> * 따라서 Elasticsearch의 보존 기간 이전의 로그를 백업 및 아카이빙하고 다시 아카이빙한 로그를 복구하여 로드하는 파이프라인을 TeamCity에 구축
>   + TeamCity에서 파이썬 스크립트로 Elasticsearch RESTful API에 접근하여 백업 및 복구하도록 파이프라인 구성
>   + curator로는 이처럼 살짝 복잡한 작업을 하기엔 무리였음
> * 마찬가지로 아카이빙되는 압축 파일을 무한정 보관할 수 없어서 최근 3년치만 보관하고 그 이후 압축 파일은 삭제하도록 결정
> * 백업 및 아카이빙한 로그 인덱스는 Elasticsearch에서 삭제
> * 로그 삭제 및 백업 파이프라인 구축 이후 파일시스템 용량 부족으로 알람이 온 경우 없이 잘 운용함
> #### 공간 정보 쿼리 성능 분석을 위한 로그 시스템 이용 사례
> * 게임 서버 컨텐츠 쪽에서 NPC가 이동하거나 무기를 다룰 때를 위해 공간 정보가 필요했음
> * 공간 정보는 (UE5 클라이언트에서) 미리 맵을 구워서 파일로 export하고 서버가 실행 시 로드하는 것으로 알고 있음
> * 서버가 공간 정보(시스템)에게 NPC의 어떤 동작을 수행하기 위해 쿼리를 날림 (수학/물리적 연산 쿼리)
> * NPC 개수가 많을수록 쿼리 개수가 증가하고 쿼리 비용 자체도 비싼 편이었음
> * 따라서 이 부분에 대한 성능 측정은 불가피했고 공간 정보 쪽 작업을 진행하면서 주기적으로 성능 측정을 수행
> * 퍼포먼스 측정을 위해 서버 내에서 테스트를 돌려 수치를 파일로 남김
> * 그냥 수치만 보고서 한눈에 파악하기가 어려워서 로그 시스템(Elasticsearch)을 활용
> * 퍼포먼스 측정 수치만 따로 Fluentd에서 파싱하여 별도 로그 인덱스로 남기고 Kibana에서 해당 로그 인덱스에 대해 만든 대시보드 안에서 그래프 등으로 통계를 출력
>   + 평균, 표준 편차 등 여러 공식을 적용했던 것으로 기억함
> * 퍼포먼스 측정 수치를 다양한 방법으로 분석할 수 있도록 로그 시스템을 잘 활용하여 지원한 사례

</details>

## 2.3 Monitoring System
### 사용 목적
* 실에서 운영하는 장비들과 서비스(소프트웨어)들을 모니터링
### 구성 요소
#### Metrics exporter
##### Node exporter (Windows/Linux)
* 호스트 종료 및 리소스(CPU, Memory, Disk) 과용 탐지
  + 특히 각 호스트의 디스크 부족 현상에 의한 불능 상태를 미리 탐지
##### MySQL exporter
* MySQL 메모리 사용량, 클라이언트 연결, slow queries, table locks 등
  + DBA가 주로 이용
##### Elasticsearch exporter
* 클러스터 현황, JVM 메모리 사용량, cpu 및 메모리 사용량 등
#### Metrics db (Prometheus)
* PromQL로 지표를 다양하게 조회하거나 알람 조건을 설정
#### Alerter (AlertManager)
* 장비의 리소스 과용 시 teams 채널에 메일로 알람 전송
#### Metrics viewer (Grafana)
* Prometheus로부터 다양한 지표를 그래프 등으로 시각화하여 출력
* 아래 사진은 여러 호스트에 설치된 Node Exporter의 지표를 모아서 출력하여 호스트 모니터링을 위한 페이지 (빌드 장비 모니터링)

&emsp;&emsp;![361664543-bca9e331-bc2b-4868-88b0-b00ded345c93](https://github.com/user-attachments/assets/226a9767-23e5-4548-84b3-3a09d23f55a3)

### Trouble shooting

<details>
<summary>Trouble shooting 사례 보기</summary>

> #### 장비 다운이나 리소스 사용량 감시
> * prometheus가 주기적으로 node exporter에 접근하여 지표를 수집해가는 구조
> * node exporter가 설치된 장비가 다운될 경우 해당 node exporter에 접근할 수 없게 되고 이 때부터 타이머를 가동
> * 설정된 시간이 지나도록 node exporter에 접근할 수 없게 되면 alert manager를 통해 팀즈로 알람 전송
> * 이를 통해 운영 중인 장비들이 정상 동작하는지 일일이 감시할 필요 없이 알람을 통해 문제가 발생하는 장비들만 모니터링
>   + 또는 Grafana 페이지에서 통합적으로 모니터링 가능
> * 또한 장비의 디스크 사용량도 설정된 사용량(%) 이상에 도달하면 알람을 전송
> * 특히 빌드 장비에서 CI/CD를 위해 주기적인 파일 sync가 필요하며 이로 인해 디스크 사용량이 과용되는 현상을 방지
>   + 클라이언트 리소스 파일 증가와 UE5의 installed 엔진 때문에 이 기능을 유용하게 사용
> * 그 외 로그(EFK) 서버 장비에서 메모리 사용량이 과용됨에 따라 JVM 사용량 증가나 유틸리티 프로그램들의 문제를 예측할 수 있음
> * 또한 게임 서버 퍼포먼스 테스트 시 실행한 장비의 CPU 및 메모리 사용량 또한 측정하여 결과 지표로 사용

</details>

# 3. DevOps
* (CI에서 서버 빌드 및 병합 관련 작업은 서버팀 일부 인원, 빌드관리팀 그리고 작성자가 함께 담당함)
* (그 외 나머지 부분들인 containerization, CD는 작성자가 전담함)
## 3.1 CI
### VCS (Version control system)
* Perforce 사용 (빌드관리팀이 퍼포스 서버를 직접 운영)
* 사용자(개발자나 기획자 등)는 P4V를 사용하여 소스코드 파일이나 데이터 파일, 리소스 파일(아트) 등을 내려 받고 (sync) 추가 및 수정하여 업로드(submit) 수행
### Build & Run
* 크로스 플랫폼(Windows/Linux) 지원을 위해 CMake/Make 사용
* CMake를 사용하여 프로젝트 별로 Visual studio 프로젝트 파일 및 make 파일 생성
  + 윈도우 환경에서 Visual studio를 사용하여 작업 (주로 디버깅 용도로 사용)
  + Visual studio code도 사용 (주로 소스코드 작업할 때 사용)
* Make를 사용하여 빌드
  + Linux 환경은 원래 Make 명령어로 빌드가 가능하지만 윈도우 환경에서는 Visual studio 프로젝트에서 빌드 (또는 msbuild.exe 사용)
  + 빌드를 Make로 통일하기 위해 Make 스크립트를 사용하여 빌드
* 컴파일러는 윈도우 환경에서 MSVC를, 리눅스 환경에서 Clang 사용
* 실행
  + 서버 소스코드를 내려받아서 빌드를 돌리지 않아도 기본적으로 서버 CI에 의해 자동 서밋되는 서버 바이너리들로 서버 실행이 가능
  + 서버 실행에만 필요한 설치 프로그램으로 vc_redist.x64(2022), mysql-odbc-connector(v8.0.28)가 있음
### TeamCity
* Teamcity에서 job(build configuration)으로 서버 별로 빌드 CI를 돌림
  + Teamcity에 VCS로 P4를 지정하고 P4에서 각 서버별 소스 파일들 변경이 생기는지 감시하다가 변경이 발생하면 Teamcity가 트리거링 되어 CI 빌드 수행
  + CI로 빌드한 결과물들을 artifacts에 저장하고 다시 VCS에 submit하여 사용자들이 일일이 서버/클라 빌드를 자리에서 하지 않더라도 최신 버전을 실행할 수 있도록 지원
### Merge
* 여러 스트림(브랜치) 간에 병합, 쿠킹 그리고 릴리즈 버전 생성은 빌드 관리팀이 전담
* 스트림은 크게 클라/서버/기획 프로그램 개발 및 데이터 세팅 환경을 위한 develop 스트림과 기획/아트 리소스 개발을 위한 internal 스트림으로 구분
* 위 스트림에 대해 매일 빌드관리팀에서 병합 작업을 수행하며 병합된 스트림에 대해 (언리얼 엔진의) 쿠킹을 수행 후 테스트 진행
* 쿠킹 테스트가 완료되면 릴리즈 버전(쿠킹 버전)을 생성 후 배포
### Containerization
#### TeamCity Agent
* 팀시티에서 실질적으로 job을 수행하는 에이전트를 도커 컨테이너로 실행
  + 팀시티 에이전트 도커 이미지는 docker hub에서 제공
* 윈도우 서버 및 우분투 등 크로스 플랫폼 에이전트 지원
* dood(docker out of docker)를 지원하여 에이전트 내부에서 도커 컨테이너 관련 작업도 수행
  + 서버 도커 이미지 빌드 시 사용
  + visualizer 도커 이미지 빌드 시 사용
  + 그 외 에이전트에서 도커 관련 작업 필요 시 사용
#### Server - Windows (Windows Server 2019)
* 서버 실행 환경으로 크로스 플랫폼 지원을 위해 윈도우 컨테이너로 실행할 수 있는 도커 이미지 생성
* mcr.microsoft.com/windows/servercore:1809 (서버 코어) 이미지를 베이스 이미지로 사용
* vc_redist.x64 및 mysql odbc driver 등을 추가로 설치하고 서버 바이너리 파일들과 데이터 파일 등을 주입
#### Server - Linux (Ubuntu 20.04)
* 서버 실행 환경으로 크로스 플랫폼 지원을 위해 리눅스 컨테이너로 실행할 수 있는 도커 이미지 생성
* ubuntu:20.04 이미지를 베이스 이미지로 사용
* unix-odbc 및 uuid 등을 추가로 설치하고 서버 바이너리 파일들과 데이터 파일 등을 주입
### Docker Image Registry
#### Private Registry
* 내부에서 편하게 사용하기 위해 도입한 docker private registry
* 서버, visualizer 등 각종 도커 이미지를 보관
#### Harbor
* NCSOFT 사내에서 공식으로 제공해주는 컨테이너 이미지 저장소
* NCCLOUD, NCKUBE 등의 환경에 배포할 경우 이 저장소를 사용해야 함
### Trouble shooting
<details>
<summary>Trouble shooting 사례 보기</summary>

> #### TeamCity 도커 컨테이너 에이전트로 일관된 CI 환경 제공
> * 팀시티 에이전트를 보통 호스트에 직접 설치하여 사용하는 케이스가 대부분
> * 빌드 환경이나 기타 CI/CD에 필요한 여러 환경을 팀시티 에이전트를 설치한 호스트에 직접 동일하게 맞춰줘야 함
> * 이 때 환경 세팅을 위한 요소들을 기록하지 않거나 스크립트화하지 않으면 누락되기 일쑤
> * 이 환경을 도커 이미지로 스크립트화해서 누락되는 요소 없이 관리
> * 언제나 일관된 CI/CD 환경을 제공하며 손쉽게 추가/제거가 가능하여 유연한 확장성 제공

</details>

## 3.2 CD
### Portainer
![portainer_logo](https://github.com/user-attachments/assets/a934d645-ef6c-4910-a3fb-5a6d042bf979)
* Container Orchestration tool (of tool)
* Docker Standalone, Docker Swarm, K8s 등을 연동하여 컨테이너 배포
  + LLL에서는 Docker Standalone, K8s만 사용
* 쿠킹 버전의 공용 서버 배포를 위해 Portainer를 사용
* 쿠킹 공용 서버 외 다른 인프라 구성 요소들도 Portainer로 배포하고 제어
  + 통계 시스템, 로그 시스템, 모니터링 시스템 등
### Docker Standalone
* 호스트에 도커 엔진을 설치하면 기본적으로 외부에 TCP로 접근할 수 있는 API를 오픈할 수 있음
* 이 도커 API를 통해서 컨테이너를 배포하거나 실행 중인 컨테이너 리스트 등의 조작이 가능
* Portainer에서 도커 API 접근 정보를 입력하여 Docker Standalone으로 등록해서 도커 이미지 배포 및 컨테이너 실행
### K8s (On-Premise)
* 사내 K8s 클라우드 플랫폼인 NCKUBE 2.0이 나오기 전에 실 내 자체적으로 구축한 K8s On-Premise 클러스터
* NCKUBE 1.0까지는 윈도우 노드와 리눅스 노드의 혼합 클러스터 서비스가 제공되지 않았었음
<details>
<summary>K8s 혼합 클러스터가 필요한 이유</summary>

> * [1.1 Server Architecture](#11-Server-Architecture) 에서 게임 서버는 리눅스 환경에서의 빌드 및 실행을 위한 포팅 작업이 완료되지 않아서 윈도우 환경에서만 실행 가능하여 혼합 클러스터가 필요했음
> * 서버 프레임워크와 그 외 다른 서버들은 비교적 규모가 작아서 서버 구현 초기부터 리눅스 환경에서의 빌드와 실행이 가능했음
> * 하지만 게임 서버의 컨텐츠 관련 코드들이 너무 방대해서 리눅스 환경에서 빌드 및 실행이 가능하도록 포팅하는 작업이 쉽게 진행되지 않았음
> * 결국 게임 서버는 윈도우 환경에서만 빌드하고 실행하는 것으로 결정함
> * 전 TD가 크로스 플랫폼을 추진하였지만 현 TD는 별로 좋아하지 않았음
>   + 현 TD는 컨텐츠 관련 작업이 더 중요하다고 판단했고 리눅스 환경에서의 프로그래밍 또한 (게임 업계 서버에서) 일반적이지 않아서 허들(비용)이라고 판단 (추측)
> * 2023 GStar 출시 준비중에 리눅스 환경에서의 빌드와 실행 지원은 중단되었고 (지스타 준비에 올인하기 위해) 이쯤 전 TD가 퇴사하면서 리눅스 환경은 지원하지 않도록 (암묵적으로) 결정

</details>

* K8s 혼합 클러스터 구축을 위해 Docker Engine을 Container Runtime으로 사용
  + Container Runtime으로 containerd, Docker, CRI-O 등이 있음
* K8s 혼합 클러스터 구축을 위해 K8s 네트워크 플러그인으로 Calico를 사용

&emsp;&emsp;![calico](https://github.com/user-attachments/assets/9dcfb022-3b4f-40dd-811a-5acf33b7c354)

<details>
<summary>Calico 네트워크 플러그인 도입 배경</summary>

* K8s 클러스터 구축 시 네트워크 플러그인 도입은 필수 (CNI, Container Network Interface를 지원하는 네트워크 플러그인)
* CNI 플러그인의 모델은 크게 VXLAN 같은, 실제 물리적인 L3 레이어 위에 가상의 L2 레이어를 만들어서 제공하는 encapsulated 모델과 그렇지 않은 unencapsulated 모델이 있음
  + encapsulated 모델은 쉽게 말해 도커 엔진이 어느 한 노드에서 실행된 컨테이너들의 통신을 위해 별도 네트워크(bridge)를 제공해주는데 이 네트워크 모델을 여러 노드에서도 공유할 수 있게 확장한 개념이라고 보면 됨
    + 각 노드에 k8s worker(kubelet)들이 네트워크 플러그인을 사용하여 이 모델을 형성해서 컨테이너에게 제공
  + unencapsulated 모델은 
  + 출처 : https://ranchermanager.docs.rancher.com/faq/container-network-interface-providers
  + encapsulated 모델 예시
    + ![encapsulated-network-0c75db46568d5b2636dad4a8c28d3cc4](https://github.com/user-attachments/assets/4e8b43c9-7f04-466b-ab9d-a70694034058)
  + unencapsulated 모델 예시
    + ![unencapsulated-network-b87922f280aa17322e6485b81855dd4a](https://github.com/user-attachments/assets/4d97915a-1d85-42f7-81a0-3ba8449eb8a6)
* LLL 서버간 통신이 반드시 같은 L2 레이어에서 동작해야 함을 보장해줄 필요가 없으므로 unencapsulated 모델을 사용
* 
### NCKUBE (K8s on Cloud)
### Ansible (GStar)
### Trouble shooting
<details>
<summary>Trouble shooting 사례 보기</summary>

> #### 윈도우 서버 컨테이너의 Hyper-v Isolation vs Process Isolation
> 

</details>
