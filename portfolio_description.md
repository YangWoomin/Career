
## 소개
* 이 문서는 저의 오픈 소스 포트폴리오인 [ZilliaxServer](https://github.com/YangWoomin/ZilliaxServer)에 대한 설명글입니다.
* 모든 내용을 설명드리기 어려우니 **데이터 신뢰성(Data Reliability)** 위주의 내용을 정리하였습니다.
  + 메모리 데이터베이스(레디스 클러스터, 이하 레디스) 및 메시지 큐(카프카 클러스터, 이하 카프카)를 사용하여 백엔드 서버(C++/Golang)간 데이터 송수신 과정에서 **데이터 유실 및 중복 방지** 처리 (논리적 수준 포함)
  + 레디스 클러스터의 여러 노드에 저장된 데이터에 대해 **결과적 일관성(Eventually Consistency)** 방법으로 데이터 일관성을 보장
* 여기서 설명하는 데이터 신뢰성 보장을 위한 시나리오는 클라이언트별로 전송한 메시지 개수를 저장하고, 전송된 메시지별로 개수를 일관적으로 저장하는 흐름으로 진행됩니다. (레디스에 저장)
* 데이터 신뢰성 위주의 테스트까지 진행하였고 성능 테스트는 아직 진행하지 못하였습니다.
* 직접 프로젝트를 빌드하고 실행이 가능하나 번거로우실테니 이 문서를 보시면서 대략적으로 프로젝트 내용을 파악하시면 되겠습니다.
* 음슴체로 적게 된 점 양해 부탁 드립니다.

## 상세 설명

### 개요

<br/>

![zilliax_server_overview](https://github.com/user-attachments/assets/fefcc9de-039b-48bf-920d-352a9d648b3f)

<br/>

### 구성 요소별 프로젝트 모듈 관계
* Client : "network_test" (C/C++)
* Producer Server : "mq_test_producer" (C/C++)
* Cache Server : Redis Cluster, "cache" 모듈에서 접근 (프로듀서 서버가 사용, C/C++)
* Message Queue : Kafka Cluster, "mq" 모듈에서 접근 (프로듀서 서버가 사용, C/C++)
* Client Message Counter : "mq_test_consumer" (Golang)
* Message Aggregator : "mq_test_consumer" (Client Message Counter와 동일 프로젝트, Golang)

### 클라이언트
* 사용자 입력을 1줄씩 받아서 프로듀서 서버에게 전송 (1번 과정)
* mtc(massive test client) 모드로 실행시 로컬의 샘플 파일(소설)을 읽어서 프로듀서 서버에게 전송

### 프로듀서 서버
* 클라이언트로부터 데이터를 전송 받으면 캐시 서버에 클라이언트 식별자(host:port)별로 메시지를 순서대로 저장 (by 네트워크 스레드풀, 비동기, 2번 과정)
  + 클라이언트 별로 메시지에 sn(sequence number) 부여 (논리적 수준의 메시지 순서 보장 및 중복 방지)
  + 레디스 클러스터에 메시지를 분산 저장하기 위해 클라이언트 식별자를 해시슬롯(hash slot)으로 사용
* 별도 스레드들이 캐시 서버로부터 적재된 메시지들을 순차적으로 로드해와서 메시지 큐에 저장 (3,4번 과정)
* 메시지 큐에 적재 완료된 메시지들의 상태를 캐시 서버에 업데이트 (5번 과정)
* 주기적인 메시지 유실(메시지 큐 적재 실패 등) 검사를 수행하여 유실된 메시지 발견시 해당 메시지부터 재전송 (논리적 수준의 메시지 유실 방지)
* 기본적으로 enable.idempotence=true 옵션을 활성화하여 메시지 중복을 방지 (물리적 수준의 메시지 중복 방지)
* acks=all 옵션으로 데이터 분산 저장 (물리적 수준의 메시지 유실 방지)

### 메시지 큐
* 프로듀서 서버로부터 client_message 토픽에 메시지 저장
* 메시지 키로 클라이언트 식별자를 사용하여 클라이언트 메시지 순서를 보장 (논리적/물리적 메시지 순서 보장)
* ISR(In-Sync Replica)은 3으로 데이터 분산 저장 및 가용성 증가 (물리적 수준의 메시지 유실 방지)
* 클라이언트 식별자를 기준으로 순서가 보장되어야 하므로 토픽의 파티션 개수는 서버가 수용할 (예상하는) 클라이언트 수만큼 생성 (100개)

### 클라이언트 메시지 카운터
* 클라이언트별로 메시지 개수를 계산하기 위한 서버
* client_message 토픽으로부터 여러 컨슈머(+프로듀서)를 사용하여 메시지 조회 (6번 과정)
* 캐시 서버에 클라이언트 식별자별 메시지 개수 저장 (7번 과정)
  + 마지막으로 연산한 메시지의 sn을 기억하고 있으며 마지막 sn의 +1에 해당하는 메시지만 처리 (논리적 수준의 메시지 순서 보장 및 중복 방지)
  + 마지막 sn+1이 아닌 메시지들은 버림 (TODO: DLQ에 보관, sn+1보다 큰 메시지들은 프로듀서 서버의 메시지 유실 감지로 재전송됨)
  + 프로듀서 서버와 마찬가지로 클라이언트 식별자를 해시슬롯으로 사용
* 소비한 메시지 오프셋 커밋과 메시지 수집기(aggregator)를 위한 메시지 적재를 트랜잭션으로 처리 (물리적 메시지 유실 방지, 8번 과정)
  + message_aggregation 토픽에 적재
* 메시지 오프셋 커밋과 메시지 적재는 한 번에 모아서 하나의 트랜잭션으로 처리 (성능 향상)

### 메시지 수집기
* 메시지별로 개수를 계산하기 위한 서버
* message_aggregation 토픽으로부터 여러 컨슈머를 사용하여 메시지 조회 (9번 과정)
* 캐시 서버에 메시지별 메시지 개수를 저장 (10번 과정)
  + 레디스 클러스터에 메시지를 분산 저장하기 위해 메시지 자체(TODO: 해시)를 해시슬롯으로 사용
  + 메시지별 클라이언트 식별자와 메시지 sn으로 메시지 중복 여부를 확인 (논리적 수준의 메시지 중복 방지)
* 소비한 메시지 오프셋 커밋을 수행 (11번 과정)

## 데이터 신뢰성 테스트

### 기본 테스트

#### 환경
<details>
<summary>details</summary>

* PC
  + Windows 10 + WSL2 (Ubuntu 24.04)
  + Intel(R) Core(TM) i7-8565U CPU @ 1.80GHz
  + 16GB RAM
  + 512G SSD
* 클라이언트
  + 클라이언트(소켓) 수 : 10
* 프로듀서 서버
  + 1대
  + acks=all
  + enable.idempotence=true
  + 그 외 나머지는 모두 기본값
* 캐시 서버 (레디스 클러스터)
  + 마스터 3대 + 복제 3대
  + Redis Insight 사용
  + 모든 설정 기본값
* 메시지 큐 (카프카 클러스터)
  + 크래프트 모드
  + 컨트롤러 3대 + 브로커 3대
  + Conduktor 사용
  + client_message 토픽 - 파티션 10개, replication factor 3, min isr 1 (옵션들은 기본값)
  + message_aggregation 토픽 - 파티션 10개, replication factor 3, min isr 1 (옵션들은 기본값)
  + 그 외 나머지는 모두 기본값
* 클라이언트 메시지 카운터 및 메시지 수집기
  + auto.offset.reset=earliest
  + enable.auto.commit=false
  + isolation.level=read_committed
  + enable.idempotence=true
  + transactional.id=[임의 문자열]
  + group.id=[임의의 문자열] (클라이언트 메시지 카운터와 메시지 수집기는 서로 다른 컨슈머 그룹)
  + 클라이언트 메시지 카운터만 프로듀서에서 트랜잭션 사용
    + 최대 100개 메시지씩 1개 트랜잭션으로 커밋하되 여유로운 상태(소비할 메시지가 없을 때)라면 바로바로 커밋
  + 프로듀서 및 컨슈머는 각각 3개씩
  + 그 외 나머지는 모두 기본값

</details>

#### 테스트 데이터
* (4422 행, 총 283052 바이트 크기) x 10 (클라이언트 수)

#### 테스트 결과
* 클라이언트 메시지 송신 결과

![image](https://github.com/user-attachments/assets/4732de75-2858-465a-868f-1ce796f598a6)

* 프로듀서 서버 메시지 수신 및 적재 결과



* client_message 토픽

* message_aggregation 토픽

* 데이터 일관성 조회

### 메시지 유실 테스트 (준비중)
#### 프로듀서 서버
* 프로듀서 버퍼가 가득 차서 유실되는 경우
* 메시지 큐로부터 메시지 저장에 실패한 경우
#### 클라이언트 메시지 카운터 & 메시지 수집기
* 컨슈머 리밸런스 유도 (컨슈머 추가/재시작(장애) 등의 사유)

## 성능 테스트 (준비중)
### 테스트 환경
### 프로듀서 서버의 메시지 수신 처리량
### 프로듀서 서버의 메시지 전송량
### 클라이언트 메시지 카운터 & 메시지 수집기의 메시지 소비량

## 더 고민해볼 것들
(준비중)
