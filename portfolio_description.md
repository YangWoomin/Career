
## 소개
* 이 문서는 저의 오픈 소스 포트폴리오인 [ZilliaxServer](https://github.com/YangWoomin/ZilliaxServer)에 대한 설명글입니다.
* 모든 내용을 설명드리기 어려우니 **데이터 신뢰성(Data Reliability)** 위주의 내용을 정리하였습니다.
  + 메모리 데이터베이스(레디스 클러스터, 이하 레디스) 및 메시지 큐(카프카 클러스터, 이하 카프카)를 사용하여 백엔드 서버(C++/Golang)간 데이터 송수신 과정에서 **데이터 유실 및 중복 방지** 처리 (논리적 수준 포함)
  + 레디스 클러스터의 여러 노드에 저장된 데이터에 대해 **결과적 일관성(Eventually Consistency)** 방법으로 데이터 일관성을 보장
* 여기서 설명하는 데이터 신뢰성 보장을 위한 시나리오는 클라이언트별로 전송한 메시지 개수를 저장하고, 전송된 메시지 별로 개수를 일관적으로 저장 (레디스에 저장)
* 데이터 신뢰성 위주의 테스트까지 진행하였고 성능 테스트는 아직 진행하지 못하였습니다.
* 직접 프로젝트를 빌드하고 실행이 가능하나 번거로우실테니 이 문서를 보시면서 대략적으로 프로젝트 내용을 파악하시면 되겠습니다.
* 음슴체로 적게 된 점 양해 부탁 드립니다.

## 상세 설명

### 개요

<br/>

![zilliax_server_overview](https://github.com/user-attachments/assets/0ddefa7a-d498-4341-aaed-1e4ca7606a6a)

<br/>

### 요소별 프로젝트 모듈 관계
* Client : "network_test"
* Producer Server : "mq_test_producer"
* Cache(Redis) Server : "cache" 모듈에서 접근 (프로듀서 서버가 사용)
* Message Queue(Kafka) : "mq" 모듈에서 접근 (프로듀서 서버가 사용)
* Client Message Counter : "mq_test_consumer"
* Message Aggregator : "mq_test_consumer" (Client Message Counter와 동일 프로젝트)

### 클라이언트
* 사용자 입력을 1줄씩 받아서 프로듀서 서버에게 전송 (1번 과정)
* mtc(massive test client) 모드로 실행시 로컬의 샘플 파일(소설)을 읽어서 프로듀서 서버에게 전송
  + 기본적으로 100개 연결(소켓)을 만들어서 샘플 파일 내용을 각각 전송 (유사 대규모 트래픽)
  + 샘플 파일은 공백 등을 제거하여 (4422 행, 총 283052 바이트 크기) x 100 개 클라이언트

### 프로듀서 서버
* 클라이언트로부터 데이터를 전송 받으면 캐시 서버에 클라이언트 식별자(host:port)별로 메시지를 순서대로 저장 (by 네트워크 스레드풀, 비동기, 2번 과정)
  + 클라이언트 별로 메시지에 sn(sequence number) 부여 (논리적 수준의 메시지 순서 보장 및 중복 방지)
  + 레디스 클러스터에 메시지를 분산 저장하기 위해 클라이언트 식별자를 해시슬롯(hash slot)으로 사용
* 별도 스레드들이 캐시 서버로부터 적재된 메시지들을 순차적으로 로드해와서 메시지 큐에 저장 (3,4번 과정)
* 메시지 큐에 적재 완료된 메시지들의 상태를 캐시 서버에 업데이트 (5번 과정)
* 주기적인 메시지 유실(메시지 큐 적재 실패 등) 검사를 수행하여 유실된 메시지 발견시 해당 메시지부터 재전송 (논리적 수준의 메시지 유실 방지)
* 기본적으로 enable.idempotence=true 옵션을 활성화하여 메시지 중복을 방지 (물리적 수준의 메시지 중복 방지)

### 메시지 큐
* 프로듀서 서버로부터 client_message 토픽에 메시지 저장
* 메시지 키로 클라이언트 식별자를 사용하여 클라이언트 메시지 순서를 보장 (논리적/물리적 메시지 순서 보장)

### 메시지 카운터
* 클라이언트별로 메시지 개수를 계산하기 위한 서버
* client_message 토픽으로부터 여러 컨슈머(+프로듀서)를 사용하여 메시지 조회 (6번 과정)
* 캐시 서버에 클라이언트 식별자별 메시지 개수 저장 (7번 과정)
  + 마지막으로 연산한 메시지의 sn을 기억하고 있으며 마지막 sn의 +1에 해당하는 메시지만 처리 (논리적 수준의 메시지 순서 보장 및 중복 방지)
  + 마지막 sn+1이 아닌 메시지들은 버림 (TODO: DLQ에 보관, sn+1보다 큰 메시지들은 프로듀서 서버의 메시지 유실 감지로 재전송됨)
  + 프로듀서 서버와 마찬가지로 클라이언트 식별자를 해시슬롯으로 사용
* 소비한 메시지 오프셋 커밋과 메시지 수집기(aggregator)를 위한 메시지 적재를 트랜잭션으로 처리 (물리적 메시지 유실 방지, 8번 과정)
  + message_aggregation 토픽에 적재
* 메시지 오프셋 커밋과 메시지 적재는 한 번에 모아서 하나의 트랜잭션으로 처리 (성능 향상)

### 메시지 수집기
* 메시지별로 개수를 계산하기 위한 서버
* message_aggregation 토픽으로부터 여러 컨슈머를 사용하여 메시지 조회 (9번 과정)
* 캐시 서버에 메시지별 메시지 개수를 저장 (10번 과정)
  + 레디스 클러스터에 메시지를 분산 저장하기 위해 메시지 자체(TODO: 해시)를 해시슬롯으로 사용
  + 메시지별 클라이언트 식별자와 메시지 sn으로 메시지 중복 여부를 확인 (논리적 수준의 메시지 중복 방지)
* 소비한 메시지 오프셋 커밋을 수행 (11번 과정)

## 테스트
(작성중)

## 더 고민해 볼 것들
(작성중)
