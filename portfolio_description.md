
## 소개
* 이 문서는 저의 오픈 소스 포트폴리오인 [ZilliaxServer](https://github.com/YangWoomin/ZilliaxServer)에 대한 설명글입니다.
* 모든 내용을 설명드리기 어려우니 **데이터 신뢰성(Data Reliability)** 위주의 내용을 정리하였습니다.
  + 메모리 데이터베이스(레디스 클러스터, 이하 레디스) 및 메시지 큐(카프카 클러스터, 이하 카프카)를 사용하여 백엔드 서버(C++/Golang)간 데이터 송수신 과정에서 **데이터 유실 및 중복 방지** 처리 (논리적 수준 포함)
  + 레디스 클러스터의 여러 노드에 저장된 데이터에 대해 **결과적 일관성(Eventually Consistency)** 방법으로 데이터 일관성을 보장
* 여기서 설명하는 데이터 신뢰성 보장을 위한 시나리오는 클라이언트별로 전송한 메시지 개수를 저장하고, 전송된 메시지별로 개수를 일관적으로 저장하는 흐름으로 진행됩니다. (레디스에 저장)
* 직접 프로젝트를 빌드하고 실행이 가능하나 번거로우실테니 이 문서를 보시면서 대략적으로 프로젝트 내용을 파악하시면 되겠습니다.
* 음슴체로 적게 된 점 양해 부탁 드립니다.

## 상세 설명

### 개요

<br/>

![zilliax_server_overview](https://github.com/user-attachments/assets/fefcc9de-039b-48bf-920d-352a9d648b3f)

<br/>

### 구성 요소별 프로젝트 모듈 관계
* Client : "network_test" (C/C++)
* Producer Server : "mq_test_producer" (C/C++)
* Cache Server : Redis Cluster, "cache" 모듈에서 접근 (프로듀서 서버가 사용, C/C++)
* Message Queue : Kafka Cluster, "mq" 모듈에서 접근 (프로듀서 서버가 사용, C/C++)
* Client Message Counter : "mq_test_consumer" (Golang)
* Message Aggregator : "mq_test_consumer" (Client Message Counter와 동일 프로젝트, Golang)

### 클라이언트
* 사용자 입력을 1줄씩 받아서 프로듀서 서버에게 전송 (1번 과정)
* mtc(massive test client) 모드로 실행시 로컬의 샘플 파일(소설)을 읽어서 프로듀서 서버에게 전송

### 프로듀서 서버
* 클라이언트로부터 데이터를 전송 받으면 캐시 서버에 클라이언트 식별자(host:port)별로 메시지를 순서대로 저장 (by 네트워크 스레드풀, 비동기, 2번 과정)
  + 클라이언트 별로 메시지에 sn(sequence number) 부여 (논리적 수준의 메시지 순서 보장 및 중복 방지)
  + 레디스 클러스터에 메시지를 분산 저장하기 위해 클라이언트 식별자를 해시슬롯(hash slot)으로 사용
* 별도 스레드들이 캐시 서버로부터 적재된 메시지들을 순차적으로 로드해와서 메시지 큐에 저장 (3,4번 과정)
* 메시지 큐에 적재 완료된 메시지들의 상태를 캐시 서버에 업데이트 (5번 과정)
* 주기적인 메시지 유실(메시지 큐 적재 실패 등) 검사를 수행하여 유실된 메시지 발견시 해당 메시지부터 재전송 (논리적 수준의 메시지 유실 방지)
* 기본적으로 enable.idempotence=true 옵션을 활성화하여 메시지 중복을 방지 (물리적 수준의 메시지 중복 방지)
* acks=all 옵션으로 데이터 분산 저장 (물리적 수준의 메시지 유실 방지)

### 메시지 큐
* 프로듀서 서버로부터 client_message 토픽에 메시지 저장
* 메시지 키로 클라이언트 식별자를 사용하여 클라이언트 메시지 순서를 보장 (논리적/물리적 메시지 순서 보장)
* ISR(In-Sync Replica)은 3으로 데이터 분산 저장 및 가용성 증가 (물리적 수준의 메시지 유실 방지)
* 클라이언트 식별자를 기준으로 순서가 보장되어야 하므로 토픽의 파티션 개수는 서버가 수용할 (예상하는) 클라이언트 수만큼 생성 (100개)

### 클라이언트 메시지 카운터
* 클라이언트별로 메시지 개수를 계산하기 위한 서버
* client_message 토픽으로부터 여러 컨슈머(+프로듀서)를 사용하여 메시지 조회 (6번 과정)
* 캐시 서버에 클라이언트 식별자별 메시지 개수 저장 (7번 과정)
  + 마지막으로 연산한 메시지의 sn을 기억하고 있으며 마지막 sn의 +1에 해당하는 메시지만 처리 (논리적 수준의 메시지 순서 보장 및 중복 방지)
  + 마지막 sn+1이 아닌 메시지들은 버림 (TODO: DLQ에 보관, sn+1보다 큰 메시지들은 프로듀서 서버의 메시지 유실 감지로 재전송됨)
  + 프로듀서 서버와 마찬가지로 클라이언트 식별자를 해시슬롯으로 사용
* 소비한 메시지 오프셋 커밋과 메시지 수집기(aggregator)를 위한 메시지 적재를 트랜잭션으로 처리 (물리적 메시지 유실 방지, 8번 과정)
  + message_aggregation 토픽에 적재
* 메시지 오프셋 커밋과 메시지 적재는 한 번에 모아서 하나의 트랜잭션으로 처리 (성능 향상)

### 메시지 수집기
* 메시지별로 개수를 계산하기 위한 서버
* message_aggregation 토픽으로부터 여러 컨슈머를 사용하여 메시지 조회 (9번 과정)
* 캐시 서버에 메시지별 메시지 개수를 저장 (10번 과정)
  + 레디스 클러스터에 메시지를 분산 저장하기 위해 메시지 자체(TODO: 해시)를 해시슬롯으로 사용
  + 메시지별 클라이언트 식별자와 메시지 sn으로 메시지 중복 여부를 확인 (논리적 수준의 메시지 중복 방지)
* 소비한 메시지 오프셋 커밋을 수행 (11번 과정)

## 데이터 신뢰성 테스트

### 기본 테스트

#### 환경
<details>
<summary>details</summary>

* PC
  + Windows 10 + WSL2 (Ubuntu 24.04)
  + Intel(R) Core(TM) i7-8565U CPU @ 1.80GHz
  + 16GB RAM
  + 512G SSD
* 클라이언트
  + 클라이언트(소켓) 수 : 10
* 프로듀서 서버
  + 1대
  + acks=all
  + enable.idempotence=true
  + 그 외 나머지 설정들은 모두 기본값
* 캐시 서버 (레디스 클러스터)
  + 마스터 3대 + 복제 3대
  + Redis Insight 사용
  + 모든 설정들은 기본값
* 메시지 큐 (카프카 클러스터)
  + KRaft 모드
  + 컨트롤러 3대 + 브로커 3대
  + Conduktor 사용
  + client_message 토픽 - 파티션 10개, replication factor 3, min isr 1 (옵션들은 기본값)
  + message_aggregation 토픽 - 파티션 10개, replication factor 3, min isr 1 (옵션들은 기본값)
  + 그 외 나머지 설정들은 모두 기본값
* 클라이언트 메시지 카운터 및 메시지 수집기
  + auto.offset.reset=earliest
  + enable.auto.commit=false
  + isolation.level=read_committed
  + enable.idempotence=true
  + transactional.id=[임의 문자열]
  + group.id=[임의의 문자열] (클라이언트 메시지 카운터와 메시지 수집기는 서로 다른 컨슈머 그룹)
  + 클라이언트 메시지 카운터만 프로듀서에서 트랜잭션 사용
    + 최대 100개 메시지씩 1개 트랜잭션으로 커밋하되 여유로운 상태(소비할 메시지가 없을 때)라면 바로바로 커밋
  + 프로듀서 및 컨슈머는 각각 3개씩
  + 그 외 나머지 설정들은 모두 기본값

</details>

#### 테스트 데이터
* (4422 행, 총 283052 바이트 크기) x 10 (클라이언트 수)

#### 테스트 결과 - 클라이언트 메시지 송신 결과

![image](https://github.com/user-attachments/assets/a5b51472-d057-4ab7-84ee-81bf8be3e687)

#### 테스트 결과 - 프로듀서 서버 메시지 수신 및 적재 결과

![image](https://github.com/user-attachments/assets/82265d6b-810e-43d6-afb5-789f9141be6e)

* "total processed message count : 44220" : 프로듀서 서버가 모든 클라이언트로부터 수신한 메시지 개수
* "msg manager finalized, total sent msg count : 44220" : 프로듀서 서버가 메시지 큐로 전송한 메시지 개수
  + ("total processed message count: 44220, size : 2830520" 이 부분이 클라이언트에서 보낸 300740 * 10과 맞지 않는 이유는 300740이 메시지 크기(4바이트)까지 포함하기 때문, (300740 - (4422 * 4)) * 10 = 2830520)

#### 테스트 결과 - client_message 토픽

![image](https://github.com/user-attachments/assets/8b24c8c8-cbd5-4fd1-bdf4-bbbf9caa917b)

#### 테스트 결과 - message_aggregation 토픽

![image](https://github.com/user-attachments/assets/e862d117-f388-41f4-8489-0b31b0a507fa)

* message_aggregation 토픽의 레코드 개수는 전송한 메시지 개수인 44220보다 많은 44780인 이유는 트랜잭션으로 적재시 트랜잭션 마커(marker)도 포함되기 때문
* 트랜잭션 마커는 다수의 파티션에 대해 트랜잭션이 커밋되었거나 중단되었다는 것을 표시하기 위해 마커 메시지를 사용
* 참고 : https://stackoverflow.com/questions/79001842/count-mismatch-in-akhq-ui-0-24-0

#### 테스트 결과 - 데이터 일관성 확인 (레디스에 최종적으로 저장된 데이터를 조회)

![image](https://github.com/user-attachments/assets/a828d276-c061-47de-bcec-91ad610cb574)

* 각 클라이언트별로 4422개의 메시지 개수 확인 가능

![image](https://github.com/user-attachments/assets/f1f75c39-92a9-4f49-9fab-5c5b3e6f004d)

* 메시지가 많아서 위에 짤렸지만 각 메시지 개수가 모두 10개로 올바른 결과 출력
* 원본에 있는 모든 레코드(행)는 다른 레코드들과 중복되지 않도록 미리 작업해둠

## 메시지 유실 테스트 (준비중)
### 프로듀서 서버
#### 프로듀서 버퍼가 가득 차서 유실되는 경우
#### 메시지 큐로부터 메시지 저장에 실패한 경우
### 클라이언트 메시지 카운터 & 메시지 수집기
#### 컨슈머 리밸런스 유도 (컨슈머 추가/재시작(장애) 등의 사유)

## 성능 테스트 (준비중)
### 기본 테스트 성능 측정
### 프로듀서 서버 설정 튜닝
### 클라이언트 메시지 카운터 & 메시지 수집기 설정 튜닝

## 더 고민해볼 것들

#### 컨슈머 서버들이 굳이 카프카 스트림즈 어플리케이션으로서 동작할 필요는 없어 보이는데 왜 그렇게 했는가?
* 사실 이 작업들은 메시지 통계만 내는 작업이므로 각자 client_message 토픽에 다른 컨슈머 그룹으로 컨슈밍해도 데이터 일관성이 보장됨
* 하지만 컨슈머 사이의 의존성으로 인해 순서가 필요할 경우를 시뮬레이션하는 것이고 실제로 순서가 필요한 작업은 아님
* 마치 결제와 아이템 지급 같은 순서 보장이 필요한 컨슈밍 작업들을 시뮬레이션하는 것으로 보아도 좋음

#### 프로듀서 서버에서 처음 레디스에 메시지 저장이 실패하고 그 이후 메시지들이 적재되면 순서가 바뀌지 않는가?
* 지금은 당장 프로듀서 서버에서 로그만 찍고 있지만 만약 순서가 매우 중요한 데이터라면 클라이언트도 서버와 연동하여 데이터 순차 처리 보장을 위한 작업을 해야 함
* 클라이언트도 메시지 적재 완료를 확인하고 유실되었음을 판단한다면 재전송하는 메커니즘 도입 필요

#### 컨슈머들은 리밸런스가 발생했을 때나 크래시 등의 예기치 않은 상황에 작업하던 내용이 유실되거나 좀비 컨슈머로서 동작해도 괜찮은가?
* 컨슈머들은 최종적으로 소비한 메시지에 대한 오프셋을 커밋하지 않는다면 다음 실행시 처리되지 않은 것으로 간주하기 때문에 재처리를 수행
* 각 컨슈머들은 레디스에 데이터를 쓸 때 작업(메시지)에 대한 sn으로 순서 보장 및 중복 처리 여부 검사를 통해 연산에 대한 멱등성을 유지함으로써 데이터는 무결성을 유지

#### 데이터 흐름의 양을 제어하고 싶다면?
* 카프카 메시지 큐를 사용할 때 중요한 점은 컨슈밍 속도가 프로듀싱 속도를 잘 따라가는지 확인하고 관리하는 것임
* 컨슈머들이 보다 적극적인 소비가 가능하도록 관련 설정이나 스레드 등의 물리적 자원을 더 투자하는 것이 가능하지만 프로듀싱하는 것을 제어하는 것으로도 어느정도 관리가 가능
* 프로듀싱하는 쪽에서 과도하게 프로듀서 버퍼에 데이터를 밀어 넣을 경우 버퍼는 금방 가득차게 되며 이를 바탕으로 프로듀서 서버는 클라이언트에게 백프레셔를 적용하여 클라이언트의 메시지 전송량을 제어하는 것도 가능

#### 왜 프로듀서 서버에서 메시지 큐에 메시지를 전달하기 전에 캐시 서버에 저장을 하는가?
* 프로듀서 서버를 stateless하게 만들고 장애로 인해 재시작을 할 경우 다른 서버 인스턴스가 이어서 메시지를 유실없이 전달하기 위함

#### 결과적 일관성이 아닌 더 엄격한 일관성 보장이 필요하다면?
* 레디스는 메모리 데이터베이스로 빠른 읽기/쓰기를 지원함으로써 2PC(2 phase commit) 등의 강력한 분산 트랜잭션 메커니즘을 지원하지 않음
* 따라서 데이터에 대한 강력한 일관성이 필요하다면 MySQL(RDB) 또는 MongoDB(NoSQL) 같은 강력한 분산 트랜잭션을 지원하는 데이터베이스를 사용해야 함


(내용들을 더 모으는 중)
